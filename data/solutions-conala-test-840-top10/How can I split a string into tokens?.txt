Query: How can I split a string into tokens?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/6532346)
 Should be one line of code 

  re.findall('[a-z]+|[\d]+', 'my t0kens')
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/38716682)
 You could split on your separator( # ) at most once and take the first part of the split. 

  from sklearn.feature_extraction.text import CountVectorizer

def tokenize(text):
    return([text.split('#', 1)[0].strip()])

text = ["first ques # on stackoverflow", "please help"]

vec = CountVectorizer(tokenizer=tokenize)
data = vec.fit_transform(text).toarray()
vocab = vec.get_feature_names()

required_list = []
for word in vocab:
    required_list.extend(word.split())
print(required_list)

#['first', 'ques', 'please', 'help']
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/6532284)
  >>> [x for x in re.split(r'\s+|(\d+)',"my t0kens") if x]
['my', 't', '0', 'kens']
  

 By using capturing parenthesis within the pattern, the tokens will also be return. Since you only want to maintain digits and not the spaces, I've left the  \s  outside the parenthesis so  None  is returned which can then be filtered out using a simple loop. 


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/38716512)
 The problem lays with your tokenizer, you've split the string into the bits you want to keep and the bits you don't want to keep, but you've not split the string into words.
Try using the tokenizer below 

  class MyTokenizer(object):
    def __call__(self,s):
        if(s.find('#')==-1):
            return s.split(' ')
        else:
            return s.split('#')[0].split(' ')
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/22886622)
 
 split the line into tokens ( line.split() ) 
 take the last three tokens ( [-3:] ) 
 strip '%' from each ( token.strip('%') ) 
 convert to float 
 

 Doing all that in a one-liner list comprehension, we get: 

  [ float(token.strip('%')) for token in line.split()[-3:] ]
=> [83.6, 95.4, 30.7]
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/2984151)
 quick hack, you can first use the .join() method to join create a string out of your list, split it at '+', re-split (this creates a matrix), then use the list() method to further split each element in the matrix to individual tokens 

  a = ['(', '2', '.', 'x', '.', '(', '3', '-', '1', ')', '+', '4', ')', '/', '3', '.', 'x', '^', '2']

b = ''.join(a).split('+')
c = []

for el in b:
    c.append(list(el))

print(c)
  

 result: 

  [['(', '2', '.', 'x', '.', '(', '3', '-', '1', ')'], ['4', ')', '/', '3', '.', 'x', '^', '2']]
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/22308986)
 It might be overkill, but you could use a few methods from  itertools . 

  list(itertools.islice(itertools.chain(s.split('-', 2), itertools.repeat('')), 3)
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/6532247)
 Try the findall method instead. 

  >>> print re.findall ('[^\d ]+', "my t0kens");
['my', 't', 'kens']
>>> print re.findall ('[\d]+', "my t0kens");
['0']
>>>
  

 . 

  >>> print re.findall('[a-zA-Z]+|\\d+', "my t0kens")
['my', 't', '0', 'kens']
>>>
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/22308852)
 You could use a simple  while  loop: 

  def three_tokens(s):
    tokens = s.split('-', 2)
    while len(tokens) < 3:
        tokens.append('')
    return tokens
  

 or extend the list with a calculated number of empty strings: 

  def three_tokens(s):
    tokens = s.split('-', 2)
    tokens.extend([''] * (3 - len(tokens)))
    return tokens
  

 or use concatenation so you can put it in the return statement: 

  def three_tokens(s):
    tokens = s.split('-', 2)
    return tokens + [''] * (3 - len(tokens))
  



