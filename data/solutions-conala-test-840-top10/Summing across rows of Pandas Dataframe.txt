Query: Summing across rows of Pandas Dataframe
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/32441609)
 based on https://stackoverflow.com/a/21244355/4589926, you may want this method: 

  import pandas as pd

def filter_data(df):
    df = df.dropna(inplace = True)
    df = df[df.apply(pd.Series.nunique, axis=1)]
    return df
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/31028591)
 Use the  axis  parameter in http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.diff.html: 

  df = pd.DataFrame(np.arange(12).reshape(3, 4), columns=list('ABCD'))
#    A  B   C   D
# 0  0  1   2   3
# 1  4  5   6   7
# 2  8  9  10  11

df.diff(axis=1)            # subtracting column wise
#    A    B   C   D
# 0  NaN  1   1   1
# 1  NaN  1   1   1
# 2  NaN  1   1   1

df.diff()                  # subtracting row wise
#    A    B     C     D
# 0  NaN  NaN   NaN   NaN
# 1  4    4     4     4
# 2  4    4     4     4
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/29218931)
 Pandas DataFrames are excellent for manipulating table-like data whose columns have different dtypes.  

 If subtracting across columns and rows both make sense, then it means all the values are the same  kind  of quantity. That  might  be an indication that you should be using a NumPy array instead of a Pandas DataFrame. 

 In any case, you can use  arr = df.values  to extract a NumPy array of the underlying data from the DataFrame. If all the columns share the same dtype, then the NumPy array will have the same dtype. (When the columns have different dtypes,  df.values  has  object  dtype). 

 Then you can compute the differences along rows or columns using http://docs.scipy.org/doc/numpy/reference/generated/numpy.diff.html: 

  import numpy as np
import pandas as pd

df = pd.DataFrame(np.arange(12).reshape(3,4), columns=list('ABCD'))
#    A  B   C   D
# 0  0  1   2   3
# 1  4  5   6   7
# 2  8  9  10  11

np.diff(df.values, axis=0)    # difference of the rows
# array([[4, 4, 4, 4],
#        [4, 4, 4, 4]])

np.diff(df.values, axis=1)    # difference of the columns
# array([[1, 1, 1],
#        [1, 1, 1],
#        [1, 1, 1]])
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/43204092)
  df2.sum(1).sum()
  

 Should be enough and skip NaNs. 

 The first  sum  is a DataFrame method that returns a Series which contains the sum for every line, then the second is summing the values on this Series. 

 NaNs are ignored by default. 

  edit : using simply  df2.sum()  should be enough 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/53005438)
 There are a couple of issues: 

 
 The main problem is your construction of  df3  has  all
three series  with dtype  object , while  df1  and  df2  have
 dtype=int  for the first two series. 
 Data in Pandas dataframes is organized and stored  by series  [column]. Therefore, type-casting is performed  by series . Hence the logic for summing across "rows and columns" is necessarily different and not necessarily consistent with regards to mixed types. 
 

 To understand what's happening with the first issue, you have to appreciate that Pandas doesn't  continually check  the most appropriate dtype is selected after each operation. This would be prohibitively expensive. 

 You can check  dtypes  for yourself: 

  print({'df1': df1.dtypes, 'df2': df2.dtypes, 'df3': df3.dtypes})

{'df1': 0     int64
        1     int64
        2    object
      dtype: object,

 'df2': 0     int64
        1     int64
        2    object
      dtype: object,

 'df3': 0    object
        1    object
        2    object
      dtype: object}
  

 You can apply conversion selectively to  df3  via an operation which checks if any null values result post-conversion: 

  for col in df3.select_dtypes(['object']).columns:
    col_num = pd.to_numeric(df3[col], errors='coerce')
    if not col_num.isnull().any():  # check if any null values
        df3[col] = col_num          # assign numeric series

print(df3.dtypes)

0     int64
1     int64
2    object
dtype: object
  

 You should then see consistent treatment. At this point, it's worth discarding your original  df3 : it's not documented anywhere that continual series type-checking  can  or  should  be applied after each operation. 

 To disregard non-numeric values when summing across rows or columns you can force conversion via  pd.to_numeric  with  errors='coerce' : 

  df = pd.DataFrame([[1,2,3],[4,5,'hey'],[7,8,9]])

col_sum = df.apply(pd.to_numeric, errors='coerce').sum()
row_sum = df.apply(pd.to_numeric, errors='coerce').sum(1)

print(col_sum)

0    12.0
1    15.0
2    12.0
dtype: float64

print(row_sum)

0     6.0
1     9.0
2    24.0
dtype: float64
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/50427962)
 An alternative would be to use dummy variables for each column and then take their sum: 

  pd.get_dummies(qualityOfLife_df.loc[:, 'Crime':'Jobs']).groupby(lambda x: x.split('_')[1], axis=1).sum()
Out: 
   Down  Same  Up
0     0     1   2
1     1     0   2
2     3     0   0
  

 I'd expect this to be more efficient if you have large number of rows. 


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/15054679)
 Can't tell if you want the aggregate numbers (in which case Andy's solution is what you want), or if you want it transformed back into the original dataframe. If it's the latter, you can use  transform  

  In [33]: cols = ['PetalLength', 'PetalWidth']

In [34]: transformed = grouped_iris[cols].transform(sum).sum(axis=1)

In [35]: iris['SumLengthWidth'] = transformed

In [36]: iris.head()
Out[36]: 
   SepalLength  SepalWidth  PetalLength  PetalWidth         Name  SumLengthWidth
0          5.1         3.5          1.4         0.2  Iris-setosa            85.4
1          4.9         3.0          1.4         0.2  Iris-setosa            85.4
2          4.7         3.2          1.3         0.2  Iris-setosa            85.4
3          4.6         3.1          1.5         0.2  Iris-setosa            85.4
4          5.0         3.6          1.4         0.2  Iris-setosa            85.4
  

  Edit : General case example 

 In general, for a dataframe  df , aggregating the groupby with  sum  gives you the sum of each group  

  In [47]: df
Out[47]: 
  Name  val1  val2
0  foo     6     3
1  bar    17     4
2  foo    16     6
3  bar     7     3
4  foo     6    13
5  bar     7     1

In [48]: grouped = df.groupby('Name')

In [49]: grouped.agg(sum)
Out[49]: 
      val1  val2
Name            
bar     31     8
foo     28    22
  

 In your case, you're interested in summing these across the rows: 

  In [50]: grouped.agg(sum).sum(axis=1)
Out[50]: 
Name
bar     39
foo     50
  

 But that only gives you 2 numbers; 1 for each group. In general, if you want those two numbers projected back onto the  original  dataframe, you want to use  transform : 

  In [51]: grouped.transform(sum)
Out[51]: 
   val1  val2
0    28    22
1    31     8
2    28    22
3    31     8
4    28    22
5    31     8
  

 Notice how these values are the exact same as the values produced by  agg ,  but  that it has the same dimensions as the original  df . Notice also how every other value is repeated, since rows [0, 2, 4] and [1, 3, 5] are the same groups. In your case, you want the sum of the two values, so you'd sum this across the rows. 

  In [52]: grouped.transform(sum).sum(axis=1)
Out[52]: 
0    50
1    39
2    50
3    39
4    50
5    39
  

 You now have a series that's the same length as the original dataframe, so you can assign it back as a column (or do what you like with it): 

  In [53]: df['val1 + val2 by Name'] = grouped.transform(sum).sum(axis=1)

In [54]: df
Out[54]: 
  Name  val1  val2  val1 + val2 by Name
0  foo     6     3                   50
1  bar    17     4                   39
2  foo    16     6                   50
3  bar     7     3                   39
4  foo     6    13                   50
5  bar     7     1                   39
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/54527283)
 use  groupby  with  sum  

  df = df.groupby('Fruit').sum()
print(df)
  

 Outputs 

  Fruit      Count     
Apple      25
Banana     19
Pear       28
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/54527233)
 How about  groupby  with  sum() ? e.g  df.groupby(['Fruit'])['Count'].sum()  

  import pandas as pd
df = pd.DataFrame([["Apple", 10], ["Pear", 20], ["Apple", 5], ["Banana", 7], ["Banana", 12], ["Pear", 8], ["Apple", 10]], columns=["Fruit", "Count"])
df = df.groupby(['Fruit'])['Count'].sum()
print(df)
  

  Output:  

  Fruit
Apple     25
Banana    19
Pear      28
  



