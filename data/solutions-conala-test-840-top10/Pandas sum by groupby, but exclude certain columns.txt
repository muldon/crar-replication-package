Query: Pandas sum by groupby, but exclude certain columns
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/32751412)
 You can select the columns of a groupby: 

  In [11]: df.groupby(['Country', 'Item_Code'])[["Y1961", "Y1962", "Y1963"]].sum()
Out[11]:
                       Y1961  Y1962  Y1963
Country     Item_Code
Afghanistan 15            10     20     30
            25            10     20     30
Angola      15            30     40     50
            25            30     40     50
  

  Note that the list passed must be a subset of the columns otherwise you'll see a KeyError.  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/39937937)
 If you are looking for a more generalized way to apply to many columns, what you can do is to build a list of column names and pass it as the index of the grouped dataframe. In your case, for example: 

  columns = ['Y'+str(i) for year in range(1967, 2011)]

df.groupby('Country')[columns].agg('sum')
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/32751357)
 The  agg  function will do this for you.  Pass the columns and function as a dict with column, output: 

  df.groupby(['Country', 'Item_Code']).agg({'Y1961': np.sum, 'Y1962': [np.sum, np.mean]})  # Added example for two output columns from a single input column
  

 This will display only the group by columns, and the specified aggregate columns.  In this example I included two agg functions applied to 'Y1962'. 

 To get exactly what you hoped to see, included the other columns in the group by, and apply sums to the Y variables in the frame: 

  df.groupby(['Code', 'Country', 'Item_Code', 'Item', 'Ele_Code', 'Unit']).agg({'Y1961': np.sum, 'Y1962': np.sum, 'Y1963': np.sum})
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/53860087)
 Using  GroupBy  +  transform  with  sum , followed by Boolean indexing: 

  res = df[df.groupby('Group')['Values'].transform('sum') > 0]
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/44035946)
 This is alternative solution using  groupby  range of length of the dataframe. 

  Two columns  using  agg  

  df.groupby(np.arange(len(df))//6).agg(lambda x: {'date': x.date.iloc[0], 
                                                 'value': x.value.sum()})
  

  Multiple columns  you can use  first  (or  last ) for date and  sum  for other columns. 

  group = df.groupby(np.arange(len(df))//6)
pd.concat((group['date'].first(), 
           group[[c for c in df.columns if c != 'date']].sum()), axis=1)
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/46561147)
 Use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html by multiplied columns: 

  x4['sum_column'] = x4['Prob'].mul(x4['Ef']).groupby(x4['ID']).transform('sum')
x4 = x4.drop(['Ef','Prob', 'Rand'], axis=1)
print (x4)
    ID  W   EC  sum_column
0  101  2  0.0        2.00
1  101  2  0.0        2.00
2  102  3  0.0        0.00
3  103  4  0.0        0.00
4  104  5  1.6        0.25
5  105  6  2.0        0.29
  

 If order of columns is important use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.insert.html: 

  x4.insert(1, 'sum_column',  x4['Prob'].mul(x4['Ef']).groupby(x4['ID']).transform('sum'))
x4 = x4.drop(['Ef','Prob', 'Rand'], axis=1)
print (x4)
    ID  sum_column  W   EC
0  101        2.00  2  0.0
1  101        2.00  2  0.0
2  102        0.00  3  0.0
3  103        0.00  4  0.0
4  104        0.25  5  1.6
5  105        0.29  6  2.0
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/45977059)
 Use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html with  rename  columns: 

  d = {'order_id':'counts','fee':'sum'}
df = test.groupby('userid').agg({'order_id':'count', 'fee':'sum'})
         .rename(columns=d)
         .reset_index()
print (df)
   userid  sum  counts
0       1    8       3
1       2    4       2
  

 But better is aggregate by http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html, because http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.count.html is used if need exclude  NaN s: 

  df = test.groupby('userid')
         .agg({'order_id':'size', 'fee':'sum'})
         .rename(columns=d).reset_index()
print (df)
   userid  sum  counts
0       1    8       3
1       2    4       2
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/46677466)
 You're looking for a  groupby  on  member_no  +  max . 

  df = df.groupby('member_no', as_index=False).max()
print(df)
   member_no  data_1  data_2  data_3  dat_1  dat_2  other_1  other_2
0          1       1       3       0      0      1        1        1
1          2       0       1       5      1      0        1        1
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/47803153)
 As pointed out in the comments, you apply  sum  on the wrong axis. If you want to exclude columns from the sum, you can use  drop  (which also accepts a list of column names which might be handy if you want to exclude columns at e.g. index 0 and 3; then  iloc  might not be ideal) 

  df.drop('A', axis=1).sum(axis=1)
  

  

  J    10
K    11
L    22
  

 Also @ayhan's solution in the comments works fine. 



