Query: How do I remove dicts from a list with duplicate fields in python?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/11114981)
 The following function's list comprehension should solve your problem. 

  def f(seq):
    s = set()
    return [x for x in seq if x['id'] not in s and not s.add(x['id'])]
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/11114394)
 Dump them into another dictionary, then pull them out after. 

  dict((x['id'], x) for x in L).values()
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/41704662)
 To remove duplicates from a list of dicts: 

  list_of_unique_dicts = []
for dict_ in list_of_dicts:
    if dict_ not in list_of_unique_dicts:
        list_of_unique_dicts.append(dict_)
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/47184312)
  def remove_entries(d, k):
    if k in d:
        del d[k]
    for value in d.values():
        if isinstance(value, dict):
            remove_entries(value, k)
  

 Here's a pretty basic recursive function for doing it 

 Edit: 

 If you also want to handle lists nested in dicts, nested in lists, etc, something like the below will owrk. 

  def remove_from_dict_in_list(l, k):
    for i in l:
        if isinstance(i, list):
            remove_from_dict_in_list(i, k)
        elif isinstance(i, dict):
            remove_entries(i, k)

def remove_entries(d, k):
    if k in d:
        del d[k]
    for value in d.values():
        if isinstance(value, dict):
            remove_entries(value, k)
        elif isinstance(value, list):
            remove_from_dict_in_list(value, k)
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/5668572)
 There may be a more pythonic way of doing this but this is the basic pseudocode: 

  def is_duplicate(a,b):
  if a['name'] == b['name'] and a['cost'] == b['cost'] and abs(int(a['cost']-b['cost'])) < 2:
    return True
  return False

newlist = []
for a in oldlist:
  isdupe = False
  for b in newlist:
    if is_duplicate(a,b):
      isdupe = True
      break
  if not isdupe:
    newlist.append(a)
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/1730308)
 I always prefer to work with objects instead of dicts, if the fields are the same for every item. 

 So, I define a class: 

  class rule(object):
    def __init__(self, file, line, rule):
        self.file = file
        self.line = line
        self.rule = rule

    #Not a "magic" method, just a helper for all the methods below :)
    def _tuple_(self):
        return (self.file, self.line, self.rule)

    def __eq__(self, other):
        return cmp(self, other) == 0

    def __cmp__(self, other):
        return cmp(self._tuple_(), rule._tuple_(other))

    def __hash__(self):
        return hash(self._tuple_())

    def __repr__(self):
        return repr(self._tuple_())
  

 Now, create a list of these objects, and sort it.  ruledict_list  can be the example data in your question. 

  rules = [rule(**r) for r in ruledict_list]
rules.sort()
  

 Loop through the (sorted) list, removing unique objects as we go. Finally, create a set, to remove duplicates. The loop will also remove one of each duplicate object, but that doesn't really matter. 

  pos = 0
while(pos < len(rules)):
    while pos < len(rules)-1 and rules[pos] == rules[pos+1]:
        print "Skipping rule %s" % rules[pos]
        pos+=1
    rules.pop(pos)
rule_set = set(rules)
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/41704996)
 If the order in the result is not important, you can use a set to remove the duplicates by converting the dicts into frozen sets: 

  def remove_dict_duplicates(list_of_dicts):
    """
    Remove duplicates.
    """
    packed = set(((k, frozenset(v.items())) for elem in list_of_dicts for
                 k, v in elem.items()))
    return [{k: dict(v)} for k, v in packed]
  

 This assumes that all values of the innermost dicts are hashable. 

 â€‹Giving up the order yields potential speedups for large lists.
For example, creating a list with 100,000 elements: 

  inner = {'asndb_prefix': '50.999.0.0/16',
         'cidr': '50.999.0.0/14',
         'cymru_asn': '14618',
         'cymru_country': 'US',    
         'cymru_owner': 'AMAZON-AES - Amazon.com, Inc., US',    
         'cymru_prefix': '50.16.0.0/16',    
         'ip': '50.16.221.xxx',    
         'network_id': '50.16.xxx.0/24',    
         'pyasn_asn': 14618,    
          'whois_asn': '14618'}

large_list = list_of_dicts + [{x: inner} for x in range(int(1e5))]
  

 It takes quite a while checking for duplicates in the result list again and again: 

  def remove_dupes(list_of_dicts):
    """Source: answer from wim
    """ 
    list_of_unique_dicts = []
    for dict_ in list_of_dicts
        if dict_ not in list_of_unique_dicts:
            list_of_unique_dicts.append(dict_)
    return list_of_unique_dicts

%timeit  remove_dupes(large_list
1 loop, best of 3: 2min 55s per loop
  

 My approach, using a set is a bit faster:   

  %timeit remove_dict_duplicates(large_list)
1 loop, best of 3: 590 ms per loop
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/54890517)
 With the help of a function to keep track of duplicates, you can use some list comprehension: 

  def remove_duplicates(old_list, cols=('a', 'b', 'c')):
    duplicates = set()

    def is_duplicate(item):
        duplicate = item in duplicates
        duplicates.add(item)
        return duplicate

    return [x for x in old_list if not is_duplicate(tuple([x[col] for col in cols]))]
  

 To use: 

  >>> remove_duplicates(some_list_of_dicts)
[
    {'a': 1, 'c': 1, 'b': 1, 'e': 4, 'd': 2}, 
    {'a': 1, 'c': 2, 'b': 1, 'e': 3, 'd': 2}, 
    {'a': 1, 'c': 3, 'b': 1, 'e': 3, 'd': 2}, 
    {'a': 1, 'c': 4, 'b': 1, 'e': 3, 'd': 2}
]
  

 You can also provide different columns to key on: 

  >>> remove_duplicates(some_list_of_dicts, cols=('a', 'd'))
[
    {'a': 1, 'c': 1, 'b': 1, 'e': 4, 'd': 2}, 
    {'a': 1, 'c': 1, 'b': 1, 'e': 1, 'd': 5}, 
    {'a': 1, 'c': 1, 'b': 1, 'e': 8, 'd': 7}, 
    {'a': 1, 'c': 1, 'b': 1, 'e': 6, 'd': 9}
]
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/22876018)
 If the order matters, then you can use  http://docs.python.org/2/library/collections.html#collections.OrderedDict to collect all the items, like this 

  from collections import OrderedDict
print OrderedDict((d["key"], d) for d in my_list).values()
  

 And if the order doesn't matter, you can use a normal dictionary, like this 

  print {d["key"]:d for d in my_list}.values()
  



