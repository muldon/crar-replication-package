Query: Iterate through words of a file in Python
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/21717882)
 You really should consider using https://wiki.python.org/moin/Generators 

  def word_gen(file):
    for line in file:
        for word in line.split():
            yield word

with open('somefile') as f:
    word_gen(f)
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/19720348)
 The most flexible approach is to use list comprehension to generate a list of words: 

  with open("C:\...\...\...\record-13.txt") as f:
    words = [word
             for line in f
             for word in line.split()]

# Do what you want with the words list
  

 Which you can then iterate over, add to a  collections.Counter  or anything else you please. 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/10443092)
 This code reads the space separated file.txt 

  f = open("file.txt", "r")
words = f.read().split()
for w in words:
    print w
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/7745311)
 It really depends on your definition of  word .  

  f = file("your-filename-here").read()
for word in f.split():
    # do something with word
    print word
  

 This will use whitespace characters as word boundaries. 

 Of course, remember to properly open and close the file, this is just a quick example. 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/10445467)
 If you want to loop over an entire file, then the sensible thing to do is to iterate over the it, taking the lines and splitting them into words. Working line-by-line is best as it means we don't read the entire file into memory first (which, for large files, could take a lot of time or cause us to run out of memory): 

  with open('in.txt') as input:
    for line in input:
        for word in line.split():
            ...
  

 Note that you could use  line.split(" ")  if you want to preserve more whitespace, as  line.split()  will remove all excess whitespace. 

 Also note my use of http://docs.python.org/reference/compound_stmts.html#the-with-statement to open the file, as it's more readable and handles closing the file, even on exceptions. 

 While this is a good solution, if you are not doing anything within the first loop, it's also a little inefficient. To reduce this to one loop, we can use http://docs.python.org/library/itertools.html#itertools.chain.from_iterable and a https://www.youtube.com/watch?v=t85uBptTDYY: 

  import itertools
with open('in.txt') as input:
    for word in itertools.chain.from_iterable(line.split() for line in input):
            ...
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/22978934)
 Simply iterate over the lines in the file and use set to keep only the unique ones. 

  from itertools import chain

def unique_words(lines):
    return set(chain(*(line.split() for line in lines if line)))
  

 Then simply do the following to read all unique lines from a file and print them 

  with open(filename, 'r') as f:
    print(unique_words(f))
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/29727588)
 Consider using https://docs.python.org/2/library/stdtypes.html#file.seek: 

  with open(sys.argv[1],"rU") as queries:
    with open(sys.argv[2],"rU") as tweets:
        for query in queries:
            query_words = query.split()
            for tweet in tweets:
                tweet_words = tweet.split()
                for qword in query_words:
                    for tword in tweet_words:
                        #Comparison
            tweets.seek(0) # go back to the start of the file
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/34111526)
 Don't reinvent the wheel a  collections.Counter  will do the counting and the ordering with  .most_common  which will give you the most to least common words in that order: 

  from collections import Counter
def wordCounter(thing):
   with open(thing) as f:
       cn = Counter(w for line in f for w in line.split())
       return cn.most_common()
  

 You also don't need to read the whole file into memory, you can iterate line by line and split each line. You also have to consider punctuation which you can strip it off with  str.strip : 

  def wordCounter(thing):
    from string import punctuation
    with open(thing) as f:
        cn = Counter(w.strip(punctuation) for line in f for w in line.split())
        return cn.most_common()
  



