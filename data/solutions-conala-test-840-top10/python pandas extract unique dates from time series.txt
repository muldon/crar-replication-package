Query: python pandas extract unique dates from time series
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/14673649)
 If you have a  Series  like: 

  In [116]: df["Date"]
Out[116]: 
0           2012-10-08 07:12:22
1           2012-10-08 09:14:00
2           2012-10-08 09:15:00
3           2012-10-08 09:15:01
4    2012-10-08 09:15:01.500000
5           2012-10-08 09:15:02
6    2012-10-08 09:15:02.500000
7           2012-10-10 07:19:30
8           2012-10-10 09:14:00
9           2012-10-10 09:15:00
10          2012-10-10 09:15:01
11   2012-10-10 09:15:01.500000
12          2012-10-10 09:15:02
Name: Date
  

 where each object is a  Timestamp : 

  In [117]: df["Date"][0]
Out[117]: <Timestamp: 2012-10-08 07:12:22>
  

 you can get only the date by calling  .date() : 

  In [118]: df["Date"][0].date()
Out[118]: datetime.date(2012, 10, 8)
  

 and Series have a  .unique()  method.  So you can use  map  and a  lambda : 

  In [126]: df["Date"].map(lambda t: t.date()).unique()
Out[126]: array([2012-10-08, 2012-10-10], dtype=object)
  

 or use the  Timestamp.date  method: 

  In [127]: df["Date"].map(pd.Timestamp.date).unique()
Out[127]: array([2012-10-08, 2012-10-10], dtype=object)
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/42795010)
 You can use  dt  to access the date time object in a Series, try this: 

  pd.to_datetime(df['EventTime']).dt.date.unique().tolist()
# [datetime.date(2014, 1, 1), datetime.date(2014, 1, 2)]
  

 

  df = pd.DataFrame({"EventTime": ["2014-01-01", "2014-01-01", "2014-01-02 10:12:00", "2014-01-02 09:12:00"]})
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/14673563)
  

  (\d{4}-\d{2}-\d{2})
  

 Run it with  re.findall  function to get all matches: 

  result = re.findall(r"(\d{4}-\d{2}-\d{2})", subject)
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/45924799)
 One way is pandas series.dt.weekday 

  df['captureTime'] = pd.to_datetime(df['captureTime'])
np.sum(df['captureTime'].dt.weekday.isin([0,1,2,3,4]))
  

 It returns 4 

 You can use boolean indexing in case you need to capture the dates 

  df[df['captureTime'].dt.weekday.isin([0,1,2,3,4])]

    captureTime
0   2017-08-01 00:05:00
1   2017-08-02 00:05:00
2   2017-08-03 00:05:00
3   2017-08-04 00:05:00
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/51268652)
 You can use  pd.Series.dt.year  followed by  pd.Series.unique . 

 Timings on Python 3.6 / Pandas 0.19 below using data from @Engineero. 

  %timeit df['time'].dt.year.unique().tolist()                  # 739 µs per loop
%timeit df['time'].apply(lambda x: x.year).unique().tolist()  # 5.9 ms per loop
%timeit list(set(df['time'].dt.year.values))                  # 823 µs per loop
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/54934903)
 You can bin the data in df1 based on bins in df2 dates, 

  bins = pd.date_range(df2.date.min(), df2.date.max() + pd.DateOffset(10), freq = '10D')
labels = df2.date
df1.groupby(pd.cut(df1.date, bins = bins, right = False, labels = labels)).value.sum().reset_index()


    date        value
0   2016-03-21  20
1   2016-03-31  0
2   2016-04-10  10
3   2016-04-20  0
4   2016-04-30  10
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/33639077)
 To get the unique dates you should first http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.normalize.html (to get the  time at midnight that day , note this is  fast ), then use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html: 

  In [31]: df["Time"].dt.normalize().unique()
Out[31]:
array(['2014-12-31T16:00:00.000000000-0800',
       '2015-01-01T16:00:00.000000000-0800',
       '2015-01-02T16:00:00.000000000-0800',
       '2015-01-04T16:00:00.000000000-0800',
       '2015-01-05T16:00:00.000000000-0800'], dtype='datetime64[ns]')
  

 

 Original answer (I misread question): 

 To get the  counts  could use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.normalize.html and then use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html: 

  In [11]: df
Out[11]:
        Time
0 2015-01-01
1 2015-01-02
2 2015-01-03
3 2015-01-03
4 2015-01-05
5 2015-01-06

In [12]: df['Time'].dt.normalize().value_counts()
Out[12]:
2015-01-03    2
2015-01-06    1
2015-01-02    1
2015-01-05    1
2015-01-01    1
Name: Time, dtype: int64
  

 but perhaps the cleaner option is to resample (though I'm not sure if this is less efficient): 

  In [21]: pd.Series(1, df['Time']).resample("D", how="sum")
Out[21]:
Time
2015-01-01     1
2015-01-02     1
2015-01-03     2
2015-01-04   NaN
2015-01-05     1
2015-01-06     1
Freq: D, dtype: float64
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/38876722)
 The following should work: 

  df.groupby(['usaf', df.dat.dt.year])['dat'].apply(lambda s: s.dt.date.nunique())
  

 What I did differently is group by two levels only, then use the  nunique  method of pandas series to count the number of unique dates in each group. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/40154589)
 I think you need http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html for converting to datetime column  Allfast time  and then use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.month.html and http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.year.html: 

  print (df)
         Allfast time
0  31-Dec-14 17:55:00
1  31-Dec-14 22:55:00
2  31-Dec-14 09:30:00
3  01-Jan-15 10:55:00
4  01-Jan-15 21:15:00

print (df.dtypes)
Allfast time    object
dtype: object

df['Allfast time'] = pd.to_datetime(df['Allfast time'])
df['months'] = df['Allfast time'].dt.month
df['year'] = df['Allfast time'].dt.year
print (df)
         Allfast time  months  year
0 2014-12-31 17:55:00      12  2014
1 2014-12-31 22:55:00      12  2014
2 2014-12-31 09:30:00      12  2014
3 2015-01-01 10:55:00       1  2015
4 2015-01-01 21:15:00       1  2015

print (df.dtypes)
Allfast time    datetime64[ns]
months                   int64
year                     int64
dtype: object
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/53791067)
  You can do something like this which would be much faster than having loops:  

 1.) Create a list of dates: 

  In [34]: start_dt = '2018-01-01'
# For whole year, use periods=365
In [45]: days_list = pd.date_range(pd.to_datetime(start_dt), periods=3) 
In [59]: days_list = [i.date() for i in days_list] # Keeping only date part
  

 2.) Create a list of times: 

  In [38]: timelist = ['09:00', '10:35', '14:00', '15:50']
  

 3.) Extend the list by repeating every element in the  days_list  4 times, one for each time: 

  In [60]: import numpy as np
In [61]: days_list = np.repeat(days_list, 4)
  

 4.) Extend the timelist by multiplying it by unique dates in days_list to have the same length has days_list: 

 So, since we used  periods=3  while creating days_list. So, extending timelist by the same factor; 

  In [64]: timelist = timelist * 3
  

 5.)  

  In [65]: df = pd.DataFrame()
In [66]: df['Date'] = days_list
In [68]: df['time'] = timelist
  

 Final output: 

  In [78]: df
Out[78]: 
          Date   time
0   2018-01-01  09:00
1   2018-01-01  10:35
2   2018-01-01  14:00
3   2018-01-01  15:50
4   2018-01-02  09:00
5   2018-01-02  10:35
6   2018-01-02  14:00
7   2018-01-02  15:50
8   2018-01-03  09:00
9   2018-01-03  10:35
10  2018-01-03  14:00
11  2018-01-03  15:50
  



