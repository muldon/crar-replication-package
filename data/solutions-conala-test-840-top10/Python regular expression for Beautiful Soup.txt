Query: Python regular expression for Beautiful Soup
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/11924175)
 Know your documentation 

 http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html 

  soup.findAll(id=re.compile("para$"))
# [<p id="firstpara" align="center">This is paragraph <b>one</b>. ,
#  <p id="secondpara" align="blah">This is paragraph <b>two</b>. ]
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/11924161)
 You can use regular expressions (this example matches on the tag names, you need to adjust it so it matches on an element's id): 

  import re
for tag in soup.find_all(re.compile("^value_xxx_c_1_f_8_a_")):
    print(tag.name)
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/13572767)
 You can do this using beautiful soup's support for regular expressions. 

  import re
soup = BeautifulSoup(urllib2.urlopen(url).read(),"lxml");
for item in soup.find_all("td", { "class" : re.compile(r"^(s|sb)$") })
  

 This regular expression matches: 

 
   ^  - the start of the string  
   (s|sb)  - either the string  's'  or the string  'sb'   
   $  - the end of the string  
 


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/15165269)
 May I recommend you to Beautiful Soup.  Soup is a very good lib to parse all of your html document. 

  soup = BeatifulSoup(html_doc)
titleName = soup.title.name
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/21508370)
 You can pass a compiled regular expression object: 

  import re

...

link=soup.find_all(
    'a',
    xtclib=re.compile(r"listing_list_\d+_title_link"),
    href=True)
  

 See http://www.crummy.com/software/BeautifulSoup/bs4/doc/#a-regular-expression. 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/972928)
 Beautiful soup is definitely the way to go with a problem like this.  The code is cleaner and easier to read.  Once you have it installed, getting all the tags looks something like this. 

  from BeautifulSoup import BeautifulSoup
import urllib2

def getTags(tag):
  f = urllib2.urlopen("http://cnn.com")
  soup = BeautifulSoup(f.read())
  return soup.findAll(tag)


if __name__ == '__main__':
  tags = getTags('p')
  for tag in tags: print(tag.contents)
  

 This will print out all the values of the p tags. 


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/53856392)
 Aha, one can evidently use regular expressions for this task. From the https://www.crummy.com/software/BeautifulSoup/bs4/doc/#id16: 

 
   If you pass in a regular expression object, Beautiful Soup will filter
  against that regular expression using its search() method. This code
  finds all the tags whose names start with the letter “b”; in this
  case, the  tag and the  tag: 
 

  html_doc = """
<parent>
 <c1:doc>
   <c1:attr_a></c1:attr_a>
   <c1:attr_b></c1:attr_b>
 </c1:doc>
 <c2:doc>
   <c2:attr_a></c2:attr_a>
   <c2:attr_b></c2:attr_b>
  </c2:doc>
</parent>
"""

from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'html.parser')

import re
for tag in soup.find_all(re.compile(".attr_a")):
    print(tag.name)
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/40603422)
  import re

text="""<h3 class="heading">General Purpose</h3>"""
pattern="(<.*?>)(.*)(<.*?>)"

g=re.search(pattern,text)
g.group(2)
  

 Output: 

  'General Purpose'
  

 https://regex101.com/r/tY2l2c/1 

 If its a beautiful soup object then its even simpler to get the value. You wont need the regex. 

  from bs4 import BeautifulSoup

text="""<h3 class="heading">General Purpose</h3>"""
a=BeautifulSoup(text)
print a.select('h3.heading')[0].text
  

 Output: 

  General Purpose
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/49669665)
 Solution with https://docs.python.org/3/library/re.html: 

  from bs4 import BeautifulSoup
import re

with open(r"C:\Users\sourabhk076\Documents\CBS_1.html") as fp:
    soup = BeautifulSoup(fp.read(), 'html.parser')
  

 Let's find the div that matches the following regular expression:  background:\s*#fdc431; .  \s  matches a single Unicode whitespace character. I assumed that there can be 0 or more whitespaces so I added the  *  modifier to match 0 or more repetitions of the preceding RE. You can read more about regexes https://docs.python.org/3/library/re.html#regular-expression-syntax as they sometimes come in handy. I also recommend you this https://regex101.com/. 

  div = soup.find('div', attrs={'style': re.compile(r'background:\s*#fdc431;')})
  

 This however is equivalent to: 

  div = soup.find('div', style=re.compile(r'background:\s*#fdc431;'))
  

 You can read about that in the official documentation of https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-keyword-arguments 

 Worth reading are also the sections about the https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-filters you can provide to the  find  and other similar methods. 

 You can supply either a string, regular expression, list,  True  or a function, as shown by https://stackoverflow.com/questions/49669516/beautiful-soup-returns-nothing/49669683#49669683 in his anwser. 

 Assuming the div exists we can get its text by: 

  >>> div.text
'42263 - Unencrypted Telnet Server'
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/2830550)
 You can pass a function to http://www.crummy.com/software/BeautifulSoup/documentation.html#The%20basic%20find%20method:%20findAll%28name,%20attrs,%20recursive,%20text,%20limit,%20**kwargs%29: 

  >>> print soupHandler.findAll('div', id=lambda x: x and x.startswith('post-'))
[<div id="post-45">...</div>, <div id="post-334">...</div>]
  

 Or a regular expression: 

  >>> print soupHandler.findAll('div', id=re.compile('^post-'))
[<div id="post-45">...</div>, <div id="post-334">...</div>]
  



