Query: How to use Beautiful Soup to find a tag with changing id?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/11924175)
 Know your documentation 

 http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html 

  soup.findAll(id=re.compile("para$"))
# [<p id="firstpara" align="center">This is paragraph <b>one</b>. ,
#  <p id="secondpara" align="blah">This is paragraph <b>two</b>. ]
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/11924161)
 You can use regular expressions (this example matches on the tag names, you need to adjust it so it matches on an element's id): 

  import re
for tag in soup.find_all(re.compile("^value_xxx_c_1_f_8_a_")):
    print(tag.name)
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/47025015)
 Try by changing the string element : 

  HTMLDocument.find(id='websiteName').string.replace_with('Bar')
  

 

  from bs4 import BeautifulSoup as soup

html = """
index.html
"""
soup = soup(html, 'lxml')
result = soup.find(id='websiteName')

print(result)
# >>> index.html

result.string.replace_with('Bar')
print(result)
# >>> index.html
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/39111521)
 There are a few of issues, some tags are created using  Javascript , there are actually two tags that have a  title="Bandes-annonces" , what you see in your output is the first occurrence with obfuscated data which is  base-64  encoded with substring(s) embedded, you can see in one of the Js functions that has  AC.config = {   the following: 

   seo: {
        obfuscatedPrefix: 'ACr'
    },
  

 Each tag in the source you get back from requests contains the encoded data like  ACrL3ZACrpZGVvL3BsYXllcl9nZW5fY21lZGlhPTE5NTYxOTgyJmNmaWxtPTE0NDE4NS5odG1s  

 You can see if we replace any occurrences of the prefix  ACr  and  base-64  decode the remaining string: 

  In [113]: s = "ACrL3ZACrpZGVvL3BsYXllcl9nZW5fY21lZGlhPTE5NTYxOTgyJmNmaWxtPTE0NDE4NS5odG1s"

In [114]: s.replace("ACr", "").decode("base-64")
Out[114]: '/video/player_gen_cmedia=19561982&cfilm=144185.html'
  

 We get the href. 

 If you wanted to get the tag with the title, you could use one of the  css classes : 

  trailer = soup.find(class_="icon-play-mini", title="Bandes-annonces")
  

 which if we run the code: 

  In [117]: url = "http://www.allocine.fr/film/fichefilm_gen_cfilm=144185.html"

In [118]: page = requests.get(url)

In [119]: soup = BeautifulSoup(page.content, 'html.parser')

In [120]: trailer = soup.find(class_="icon-play-mini", title="Bandes-annonces")

In [121]: print trailer
<span class="ACrL3ZACrpZGVvL3BsYXllcl9nZW5fY21lZGlhPTE5NTYxOTgyJmNmaWxtPTE0NDE4NS5odG1s item trailer icon icon-play-mini" title="Bandes-annonces">
            Bandes-annonces
        </span>
  

 Gives you the second occurrence of the tag with the title=.. 

 Then to get the href: 

  In [122]: trailer["class"][0].replace("ACr", "").decode("base-64")
Out[122]: '/video/player_gen_cmedia=19561982&cfilm=144185.html'
  

 You can see it is not going to be very straight forward to scrape data from that site, the obfuscation is likely there for a good reason, to make scraping harder as they most likely don't want you to be doing it. 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/42396817)
 You can use the https://www.crummy.com/software/BeautifulSoup/bs4/doc/#decompose to remove the element/tag: 

  f = open(file, "r+")

soup = BeautifulSoup(f, 'html.parser')
elements = soup.find_all("div", id="jp-post-flair")
for element in elements:
  element.decompose()

f.write(str(soup))
  

 It's also worth mentioning that you can probably just use the  .find()  method because an  id  attribute should be unique within a document (which means that there will likely only be one element in most cases): 

  f = open(file, "r+")

soup = BeautifulSoup(html_doc, 'html.parser')
element = soup.find("div", id="jp-post-flair")
if element:
  element.decompose()

f.write(str(soup))
  

 

 As an alternative, based on the comments below: 

 
 If you only want to parse and modify part of the document, BeautifulSoup has a https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document that allows you to selectively parse parts of the document. 
  You mentioned that the indentations and formatting in the HTML file was being changing. Rather than just converting the  soup  object directly into a string, you can check out the relevant https://www.crummy.com/software/BeautifulSoup/bs4/doc/#output-formatters in the documentation. 

 Depending on the desired output, here are a few potential options: 

 
  soup.prettify(formatter="minimal")  
  soup.prettify(formatter="html")  
  soup.prettify(formatter=None)  
  
 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/22410466)
 To find an element by its  id : 

  div = soup.find(id="articlebody")
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/26095010)
 Try Beautiful soup selects function. It uses css selectors: 

  for span in soup_expose_html.select("span[id$=_content]"):
    print span.text
  

 the result is a list with all spans which have an id ending with _content 


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/19466808)
  from bs4 import BeautifulSoup

html = """
<html lang="en-US" xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>CATe - hj1612</title>
</td></tr></table>
</td></tr></table></td><td> </td><td align="center">
<input name="keyt" type="hidden" value="a3dvl"/>
<input type="submit" value="View"/> or 
<input type="reset" value="Reset"/>
</td>
</tr>
</html>
"""

soup = BeautifulSoup(html)

print soup.find(name="input", attrs={'name': 'keyt'})
  

 Output: 

  <input name="keyt" type="hidden" value="a3dvl"/>
  

 You can use the  find_all  function instead of  find  if you want to find multiple occurrences. As for how to use the two functions,  name  is the name of the tag that you want to find, and the  attrs  dict is what you  really  use to find things with particular attributes, in your case, the  name  attribute. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/26095233)
 How about a  findAll  solution? 
First collect all possibles id prefixes and then iterate them and get all elements 

  >>> from bs4 import BeautifulSoup
>>> import re
>>> html = """
...         <span id="07_lbl" class="lbl">Price:</span>
...         <span id="07_content" class="content">130  €</span>
...         <span id="08_lbl" class="lbl">Value:</span>
...         <span id="08_content" class="content">90000  €</span>
... 
... 
...         <span id="03_lbl" class="lbl">Price:</span>
...         <span id="03_content" class="content">130  €</span>
...         <span id="04_lbl" class="lbl">Value:</span>
...         <span id="04_content" class="content">90000  €</span>
... """
>>> 
>>> soup = BeautifulSoup(html)
>>> span_id_prefixes = [
...     span['id'].replace("_content","")
...     for span in soup.findAll('span', attrs={'id' : re.compile(r'(_content$)')})
... ]
>>> for prefix in span_id_prefixes:
...     lbl     = soup.find('span', attrs={'id' : '%s_lbl' % prefix})
...     content = soup.find('span', attrs={'id' : '%s_content' % prefix})
...     if lbl and content:
...         print lbl.text, content.text
... 
Price: 130  €
Value: 90000  €
Price: 130  €
Value: 90000  €
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/17683541)
 Find the  Births  section: 

  section = soup.find('span', id='Births').parent
  

 And then find the next unordered list: 

  births = section.find_next('ul').find_all('li')
  



