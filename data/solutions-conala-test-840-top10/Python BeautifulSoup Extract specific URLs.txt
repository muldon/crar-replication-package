Query: Python BeautifulSoup Extract specific URLs
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/15323737)
 If you're using http://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors or greater: 

  soup.select('a[href^="http://www.iwashere.com/"]')
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/15313357)
 You can match multiple aspects, including using a regular expression for the attribute value: 

  import re
soup.find_all('a', href=re.compile('http://www\.iwashere\.com/'))
  

 which matches (for your example): 

  [http://www.iwashere.com/washere.html, <a href="http://www.iwashere.com/wasnot.html" ]
  

 so any  <a>  tag with a  href  attribute that has a value that starts with the string  http://www.iwashere.com/ . 

 You can loop over the results and pick out just the  href  attribute: 

  >>> for elem in soup.find_all('a', href=re.compile('http://www\.iwashere\.com/')):
...     print elem['href']
... 
http://www.iwashere.com/washere.html
http://www.iwashere.com/wasnot.html
  

 To match all relative paths instead, use a negative look-ahead assertion that tests if the value does  not  start with a schem (e.g.  http:  or  mailto: ), or a double slash ( //hostname/path ); any such value  must  be a relative path instead: 

  soup.find_all('a', href=re.compile(r'^(?!(?:[a-zA-Z][a-zA-Z0-9+.-]*:|//))'))
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/43300480)
 if you are trying scrape urls then you should get hrefs :   

  urls = soup.find_all('a', href=True)
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/40629823)
 Have a list of urls and iterate through it. 

  from bs4 import BeautifulSoup
import requests
import pprint
import re
import pyperclip

urls = ['www.website1.com', 'www.website2.com', 'www.website3.com', .....]
#scrape elements
for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    #print titles only
    h1 = soup.find("h1", class_= "class-headline")
    print(h1.get_text())
  

 If you are going to prompt user for input for each site then it can be done this way 

  from bs4 import BeautifulSoup
import requests
import pprint
import re
import pyperclip

urls = ['www.website1.com', 'www.website2.com', 'www.website3.com', .....]
#scrape elements
msg = 'Enter Url, to exit type q and hit enter.'
url = input(msg)
while(url!='q'):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    #print titles only
    h1 = soup.find("h1", class_= "class-headline")
    print(h1.get_text())
    input(msg)
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/40630783)
 If you want to scrape links in batches. Specify a batch size and iterate over it. 

  from bs4 import BeautifulSoup
import requests
import pprint
import re
import pyperclip

batch_size = 5
urllist = ["url1", "url2", "url3", .....]
url_chunks = [urllist[x:x+batch_size] for x in xrange(0, len(urllist), batch_size)]

def scrape_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    h1 = soup.find("h1", class_= "class-headline")
    return (h1.get_text())

def scrape_batch(url_chunk):
    chunk_resp = []
    for url in url_chunk:
        chunk_resp.append(scrape_url(url))
    return chunk_resp

for url_chunk in url_chunks:
    print scrape_batch(url_chunk)
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/47776665)
 In this example, you can use BeautifulSoup to get the div with a specific id: 

  import BeautifulSoup
soup = BeautifulSoup.BeautifulSoup(html)
div = soup.find(id="product_bullets_section")
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/47166671)
 You can use BeautifulSoup to extract src attribute of html img tag. In my example, the htmlText contains the img tag but it can use a URL too if you use urllib2. 

  For URLs  

  from BeautifulSoup import BeautifulSoup as BSHTML
import urllib2
page = urllib2.urlopen('http://www.youtube.com/')
soup = BSHTML(page)
images = soup.findAll('img')
for image in images:
    #print image source
    print image['src']
    #print alternate text
    print image['alt']
  

    

  from BeautifulSoup import BeautifulSoup as BSHTML
htmlText = """  """
soup = BSHTML(htmlText)
images = soup.findAll('img')
for image in images:
    print image['src']
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/5331504)
  import urllib2
import BeautifulSoup
import re

Newlines = re.compile(r'[\r\n]\s+')

def getPageText(url):
    # given a url, get page content
    data = urllib2.urlopen(url).read()
    # parse as html structured document
    bs = BeautifulSoup.BeautifulSoup(data, convertEntities=BeautifulSoup.BeautifulSoup.HTML_ENTITIES)
    # kill javascript content
    for s in bs.findAll('script'):
        s.replaceWith('')
    # find body and extract text
    txt = bs.find('body').getText('\n')
    # remove multiple linebreaks and whitespace
    return Newlines.sub('\n', txt)

def main():
    urls = [
        'http://www.stackoverflow.com/questions/5331266/python-easiest-way-to-scrape-text-from-list-of-urls-using-beautifulsoup',
        'http://stackoverflow.com/questions/5330248/how-to-rewrite-a-recursive-function-to-use-a-loop-instead'
    ]
    txt = [getPageText(url) for url in urls]

if __name__=="__main__":
    main()
  

 It now removes javascript and decodes html entities. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/14552077)
 You could use the BeautifulSoup module: 

  from bs4 import BeautifulSoup

soup = BeautifulSoup('your html')
elements = soup.findAll('a')

for el in elements:
    print el['href']
  

 If not - just use regexp: 

  import re

expression = re.compile(r'http:\/\/*')
m = expression.search('your string')

if m:
    print 'match found!'
  

 This would match also the urls within     tags, but you can tweak my solution easily to only find urls within  <a />  tags 



