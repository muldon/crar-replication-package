Query: pandas DataFrame filter regex
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/15333283)
 Use https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html instead: 

  In [10]: df.b.str.contains('^f')
Out[10]: 
0    False
1     True
2     True
3    False
Name: b, dtype: bool
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/31076657)
 Multiple column search with dataframe: 

  frame[frame.filename.str.match('*.'+MetaData+'.*') & frame.file_path.str.match('C:\test\test.txt')]
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/48884429)
 Write a Boolean function that checks the regex and use apply on the column 

  foo[foo['b'].apply(regex_function)]
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/36565655)
 You can use https://docs.python.org/2/howto/regex.html#lookahead-assertions  (?!...) : 

  print df.filter(regex='^(?!.*Confidence).*$')
   Var_1_Reading  Var_2_Reading
0            1.0            1.0
1            2.0            2.0
2            3.0            3.0
3           10.0           10.0
4            0.1            0.1
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/54031740)
 Filter the DataFrame using a Boolean mask made from the given column and regex pattern as follows:
   df[df.column_name.str.contains('^[\d]*', regex=True)]  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/45861064)
 You could use  df.filter  with a regex: 

  In [246]: df.filter(regex=re.compile('^a$', re.I))
Out[246]: 
   A
0  1
1  2
2  3
  

 For your purpose, you'd use: 

  def select_f(row):
    return row.filter(regex=re.compile('^a$', re.I)).iloc[0]
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/54032814)
 Simplest and easiest to use with regex  df.filter : 

 You can refer the http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.filter.html 

  Actual DataFrame:  

  >>> df
   A_x  B_y  C_x  D_y
0    8    9    5    1
1    3    4    0    6
2    9    7    0    4
3    6    7    5    9
4    4    3    7    5
5    6    1    6    9
6    5    4    5    4
7    8    3    0    1
8    7    4    4    4
9    9    2    4    4
  

  Apply  dataFrame.filter  :  

  >>> df1 = df.filter(regex='_x')
>>> df2 = df.filter(regex='_y')
  

  Your Splitted New DataFrame  df1   

  >>> df1
   A_x  C_x
0    8    5
1    3    0
2    9    0
3    6    5
4    4    7
5    6    6
6    5    5
7    8    0
8    7    4
9    9    4
  

  Your Splitted New DataFrame  df2   

  >>> df2
   B_y  D_y
0    9    1
1    4    6
2    7    4
3    7    9
4    3    5
5    1    9
6    4    4
7    3    1
8    4    4
9    2    4
  

  OR :  DataFrame.filter  with  regex  which uses  re.search  under the hood.  

  >>> df1 = df.filter(regex='x$', axis=1)
>>> df2 = df.filter(regex='y$', axis=1)
  

  OR:   DataFrame.filter  with Parameter  like   

  df1, df2 = df.filter(like='_x'), df.filter(like='_y')
  

  OR: Using  dataFrame.loc  +  contains   

  df1 = df.loc[:, df.columns.str.contains('_x')]
df2 = df.loc[:, df.columns.str.contains('_x')]
  

  OR: Using  dataFrame.loc  +  map  with  lambda  +  endswith   

  df1 = df.loc[:,df.columns.map(lambda x: x.endswith('_x'))]
df2 = df.loc[:,df.columns.map(lambda x: x.endswith('_y'))]
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/35727164)
 So it looks like part of my problem with  filter  was that I was using an outdated version of pandas. After updating I no longer get the  TypeError . After some playing around, it looks like I can use  filter  to fit my needs. Here is what I found out. 

 Simply setting  df.filter(regex='string')  will return the columns which match the regex. This looks to do the same as  df.filter(regex='string', axis=1) . 

 To search the index, I simply need to do  df.filter(regex='string', axis=0)  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/43643591)
 You've several options, here's a couple: 

 1 -  filter  with  like : 

  df.filter(like='alp')
  

 2 -  filter  with  regex : 

  df.filter(regex='alp')
  



