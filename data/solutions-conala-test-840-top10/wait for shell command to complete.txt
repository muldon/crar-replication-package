Query: wait for shell command to complete
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/43691639)
 The issue is that nothing waits for the process to complete; you only wait for it to start. 

  async def run_async(loop = ''):
    cmd = 'sudo long_running_cmd --opt1=AAAA --opt2=BBBB'

    print ("[INFO] Starting script...")
    process = await asyncio.create_subprocess_shell(cmd1, stdin = PIPE, stdout = PIPE, stderr = STDOUT)
    await process.wait()
    print("[INFO] Script is complete.")
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/39722419)
 One possible solution, looks like its running in same shell: 

  subprocess.Popen('echo start;echo mid;echo end', shell=True)
  

 Note - If you pass your command as a string then shell has to be True
Note - This is working on linux only, you may have to find something similar way out on windows.  

 . 

 From python doc -  

 
   On Unix with shell=True, the shell defaults to /bin/sh. If args is a
  string, the string specifies the command to execute through the shell.
  This means that the string must be formatted exactly as it would be
  when typed at the shell prompt. 
 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/24779416)
  Popen  by default only executes  executables ,  not shell command lines .
When you pass the list of arguments to  Popen  they should call  one  executable with its arguments: 

  import subprocess

proc = subprocess.Popen(['ps', 'aux'])
  

 Also note that you should  not  use  str.split  to split a command, because: 

  >>> "ps aux | grep 'java -jar'     | grep -v grep | awk '//{print $2}'".split(' ')
['ps', 'aux', '|', 'grep', "'java", "-jar'", '', '', '', '', '|', 'grep', '-v', 'grep', '|', 'awk', "'//{print", "$2}'"]
  

 Note how: 

 
 The arguments that were quoted (e.g.  'java -jar')  are splitted. 
 If there is more than one consecutive space you get some empty arguments. 
 

 Python already provides a module that knows how to split a command line in a reasonable manner, it's https://docs.python.org/3.4/library/shlex.html: 

  >>> shlex.split("ps aux | grep 'java -jar'     | grep -v grep | awk '//{print $2}'")
['ps', 'aux', '|', 'grep', 'java -jar', '|', 'grep', '-v', 'grep', '|', 'awk', '//{print $2}']
  

 Note how quoted arguments were preserved, and multiple spaces are handled gracefully. Still you  cannot  pass the result to  Popen , because  Popen  will  not  interpret the  |  as a pipe by default. 

 If you want to run a shell command line (i.e. use  any  shell feature such as pipes, path expansion, redirection etc.) you must pass  shell=True . In this case you should  not  pass a list of strings as argumento to  Popen , but only  a  string that is the complete command line: 

  proc = subprocess.Popen("ps aux | grep 'java -jar' | grep -v grep | awk '//{print $2}'", shell=True)
  

 If you pass a list of strings with  shell=True  its meaning is different: the first element should be the  complete  command line, while the other elements are passed as options to the shell used. For example on my machine the default shell ( sh ) has an  -x  option that will display on  stderr  all the processes that gets executed: 

  >>> from subprocess import Popen
>>> proc = Popen(['ps aux | grep python3', '-x'], shell=True)
>>> 
username   7301  0.1  0.1  39440  7408 pts/9    S+   12:57   0:00 python3
username   7302  0.0  0.0   4444   640 pts/9    S+   12:58   0:00 /bin/sh -c ps aux | grep python3 -x
username   7304  0.0  0.0  15968   904 pts/9    S+   12:58   0:00 grep python3
  

 Here you can see that a  /bin/sh  was started that executed the command  ps aux | python3  and with an option of  -x . 

 (This is all documented in the documentation for https://docs.python.org/2/library/subprocess.html#subprocess.Popen). 

 

 This said, one way to achieve what you want is to use https://docs.python.org/2/library/subprocess.html#subprocess.check_output: 

  subprocess.check_output("ps aux | grep 'java -jar' | grep -v grep | awk '//{print $2}'", shell=True)
  

 However this isn't available in python<2.7 so you have to use  Popen  and  communicate() : 

  proc = subprocess.Popen("ps aux | grep 'java -jar' | grep -v grep | awk '//{print $2}'", shell=True, stdout=subprocess.PIPE)
out, err = proc.communicate()
  

 The alternative is to avoid using  shell=True  (which is generally a  very good thing , since  shell=True  introduces some security risks) and manually write the pipe using multiple processes: 

  from subprocess import Popen, PIPE

ps = Popen(['ps', 'aux'], stdout=PIPE)
grep_java = Popen(['grep', 'java -jar'], stdin=ps.stdout, stdout=PIPE)
grep_grep = Popen(['grep', '-v', 'grep'], stdin=grep_java.stdout, stdout=PIPE)
awk = Popen(['awk', '//{print $2}'], stdin=grep_grep.stdout, stdout=PIPE)
out, err = awk.communicate()

grep_grep.wait()
grep_java.wait()
ps.wait()
  

 Note that if you don't care for the standard error you can avoid specifying it. It will then inherit the one of the current process.  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/46762130)
  In :call??
Signature: call(*popenargs, **kwargs)
Source:   
def call(*popenargs, **kwargs):
    """Run command with arguments.  Wait for command to complete, then
    return the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    retcode = call(["ls", "-l"])
    """
    return Popen(*popenargs, **kwargs).wait()
File:      /usr/lib64/python2.7/subprocess.py
Type:      function
  

 call just invoke Popen,use wait() method wait the popenargs completes 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/9731891)
 Just use http://docs.python.org/library/subprocess.html#subprocess.Popen.returncode after  . . Also, tell Popen that http://docs.python.org/library/subprocess.html#popen-constructor, rather than a raw command line: 

  p = subprocess.Popen('ls | tee out.txt', shell=True, ...)
p.
print p.returncode
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/16197061)
 This is evil: 

  p = subprocess.Popen('unrar e ' + root + '/' + i + ' ' + testfolder,
        bufsize=2048, shell=True, stdin=subprocess.PIPE
  

   

  p = subprocess.Popen(['unrar', 'e', '%s/%s' % (root, i, testfolder],
        bufsize=2048, stdin=subprocess.PIPE
p.stdin.write('e'
p.wait(
if p.returncode == 0:
    pass # put code that must only run if successful here.
  

 By passing an exact array rather than a string to  Popen  and not using  shell=True , a filename with a space in it can't be interpreted as a more than one arguments, or a subshell command, or some other potentially malicious thing (think of a file with  $(rm -rf ..  in its name. 

 Then, after calling  p.wait(  (there's no need for  p.communicate(  when you aren't capturing stderr or stdout, you must check  p.returncode  to determine whether the  process was successful, and only  proceed on to delete files if  p.returncode == 0  (indicating success. 

 Your initial diagnosis, that  p.communicate(  is returning while the  unrar  process is still running, is not feasible;  p.communicate(  and  p.wait(  do not work that way. 

 

 If running across  ssh , this changes a bit: 

  import pipes # in Python 2.x; in 3.x, use shlex.quote( instead
p = subprocess.Popen(['ssh', ' '.join(
      [pipes.quote(s for s in ['unrar', 'e', '%s/%s' % (root, i, testfolder]]
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/20113077)
 It has to do with STDOUT and STDERR buffering. You should be using  subprocess.Popen  to redirect  STDOUT  and  STDERR  from your child process into your application. Then, as needed, output them. Example: 

  import subprocess

cmd = ['ls', '-la']
print('running command: "{0}"'.format(cmd))  # output the command.
# Here, we join the STDERR of the application with the STDOUT of the application.
process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
process.wait()  # Wait for the underlying process to complete.
out, err = process.communicate()  # Capture what it outputted on STDOUT and STDERR
errcode = process.returncode  # Harvest its returncode, if needed.
print(out)
print('done running command')
  

 Additionally, I wouldn't use  shell = True  unless it's really required. It forces subprocess to fire up a whole shell environment just to run a command. It's usually better to inject directly into the env parameter of Popen. 


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/46728607)
 If you are  certain  your shell scripts are not running at all, and with the first code everything works - then it  must be  the java command deadlocks or not terminates correctly using the  call()  function. 

 You can validate that by adding a dummy file creation in your bash scripts. Put it in the first line of the script, so if it is executed you'll get the dummy file created. If it's not created, that means the scripts weren't executed, probably due to something with the java execution. 

 I would have try couple things: 

 First I would return the  Popen  instead of  call . Instead of using  wait() , use  communicate() : 

 
   Interact with process: Send data to stdin. Read data from stdout and stderr, until end-of-file is reached.  Wait for process to terminate . 
   communicate()  returns a tuple  (stdoutdata, stderrdata) . 
 

  proc = subprocess.Popen(jar_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
proc.communicate()
  

 Make sure to check both streams for data (stdout and stderr). You might miss an error the java process raises. 

 Next I would try disabling the buffer by providing  bufsize=0  to  Popen . It will eliminate the option it relates to python buffering. 

 If both options still don't work, try to see if there is an exception by using  check_call() : 

  proc = subprocess.check_call(jar_command)
  

 
   Run command with arguments. Wait for command to complete. If the return code was zero then return, otherwise  raise   CalledProcessError . 
 

 These options might have the answer; if not, they would help the debugging process. Feel free to comment how this progress. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/15917639)
 I'd try something like: 

  #!/usr/bin/python
from __future__ import print_function

import shlex
from subprocess import Popen, PIPE

def shlep(cmd):
    '''shlex split and popen
    '''
    parsed_cmd = shlex.split(cmd)
    ## if parsed_cmd[0] not in approved_commands:
    ##    raise ValueError, "Bad User!  No output for you!"
    proc = Popen(parsed_command, stdout=PIPE, stderr=PIPE)
    out, err = proc.communicate()
    return (proc.returncode, out, err)
  

 ... In other words let shlex.split() do most of the work.  I would NOT attempt to parse the shell's command line, find pipe operators and set up your own pipeline.  If you're going to do that then you'll basically have to write a complete shell syntax parser and you'll end up doing an awful lot of plumbing. 

 Of course this raises the question, why not just use  Popen  with the  shell=True  (keyword) option?  This will let you pass a string (no splitting nor parsing) to the shell and still gather up the results to handle as you wish.  My example here won't process any pipelines, backticks, file descriptor redirection, etc that might be in the command, they'll all appear as literal arguments to the command.  Thus it is still safer then running with  shell=True  ... I've given a silly example of checking the command against some sort of "approved command" dictionary or set --- through it would make more sense to normalize that into an absolute path unless you intend to require that the arguments be normalized prior to passing the command string to this function. 


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/41660723)
 Both functions  call(...)  and  check_call(...)  wait by default for the command to complete.
Just for debugging, call the command through Popen and run 'abaqus' with full path 'C:\...\abaqus.exe ...'. 

  import subprocess

cmd = subprocess.Popen('abaqus job=plate ask_delete=OFF', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
out, err = cmd.communicate()
code = cmd.returncode

print(code) # should 0 to indicate the command run successfully
print(out)  # what was piped to stdout
print(err)  # what was piped to stderr
  



