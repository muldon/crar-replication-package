Query: How to replace the white space in a string in a pandas dataframe?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/42462628)
 Use  replace  method of dataframe: 

  df.replace('\s+', '_',regex=True,inplace=True)
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/41476181)
 
   To remove white spaces  : 
 

 1) To remove white space  everywhere : 

  df.columns = df.columns.str.replace(' ', '')
  

 2) To remove white space at  the beginning of string : 

  df.columns = df.columns.str.lstrip()
  

 3) To remove white space at  the end of string : 

  df.columns = df.columns.str.rstrip()
  

 4) To remove white space at  both ends : 

  df.columns = df.columns.str.strip()
  

 
   To replace white spaces with other characters   (underscore for instance): 
 

 5) To replace white space  everywhere  

  df.columns = df.columns.str.replace(' ', '_')
  

 6) To replace white space  at the beginning : 

  df.columns = df.columns.str.replace('^ +', '_')
  

 7) To replace white space  at the end : 

  df.columns = df.columns.str.replace(' +$', '_')
  

 8) To replace white space  at both ends : 

  df.columns = df.columns.str.replace('^ +| +$', '_')
  

 

 All above applies to a specific column as well, assume you have a column named  col , then just do: 

  df[col] = df[col].str.strip()  # or .replace as above
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/42462577)
  pandas  

  stack  /  unstack  with  str.replace  

  df.stack().str.replace(' ', '_').unstack()

      Person_1    Person_2     Person_3
0   John_Smith  Jane_Smith   Mark_Smith
1  Harry_Jones  Mary_Jones  Susan_Jones
  

    

  pd.DataFrame(
    np.core.defchararray.replace(df.values.astype(str), ' ', '_'),
    df.index, df.columns)

      Person_1    Person_2     Person_3
0   John_Smith  Jane_Smith   Mark_Smith
1  Harry_Jones  Mary_Jones  Susan_Jones
  

 

   time testing   
https://i.stack.imgur.com/6m7LS.png 


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/43573752)
 You can use  str.replace  method; The  http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html  method is meant to replace values literally ( exact match ), i.e, replace a value which is a whitespace in the series instead of stripping the white space from the string: 

  df_merged_1['Priority Type'] = df_merged_1['Priority Type'].str.replace(' ', '')
  

 Alternatively, you can specify  regex=True  in the replace method: 

  df_merged_1['Priority Type'] = df_merged_1['Priority Type'].replace(' ', '', regex=True)
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/21942746)
 I think  df.replace()  does the job: 

  df = pd.DataFrame([
    [-0.532681, 'foo', 0],
    [1.490752, 'bar', 1],
    [-1.387326, 'foo', 2],
    [0.814772, 'baz', ' '],     
    [-0.222552, '   ', 4],
    [-1.176781,  'qux', '  '],         
], columns='A B C'.split(), index=pd.date_range('2000-01-01','2000-01-06'))

print df.replace(r'\s+', np.nan, regex=True)
  

 Produces: 

<pre class="lang-none prettyprint-override">                    A    B   C
2000-01-01 -0.532681  foo   0
2000-01-02  1.490752  bar   1
2000-01-03 -1.387326  foo   2
2000-01-04  0.814772  baz NaN
2000-01-05 -0.222552  NaN   4
2000-01-06 -1.176781  qux NaN
  

 

 As https://stackoverflow.com/users/1351629/temak pointed it out, use  df.replace(r'^\s+$', np.nan, regex=True)  in case your valid data contains white spaces. 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/36934756)
  

  df = df.apply(lambda x: x.str.strip()).replace('', np.nan)
  

  

  df = df.apply(lambda x: x.str.strip() if isinstance(x, str) else x).replace('', np.nan)
  

 You can strip all str, then replace empty str with  np.nan . 


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/33820997)
 There is multiple space in string, so I replace all of them by  ; . Then use function http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html with parameter  skiprows=3 , which skip first 3 line of file and  names , which define names of columns. 

  import pandas
from StringIO import StringIO

pathToFile = 'test/file.txt'
f = open(pathToFile)
s = StringIO()
cur_ID = None
for ln in f:
    #replace multiply spaces to one ;
    ln = ';'.join(ln.split())
    if  ln.startswith('BD;'):
        ln = ln.replace('BD;', 'BD ') 
    if  (ln.startswith('19')) | (ln.startswith('20')):
        continue;        
    s.write(ln + '\n')
s.seek(0)

df = pandas.read_csv(s, skiprows=3, sep=';', names=['0','1','2','3','TOT'])
  



  print df
#                 0       1         2        3      TOT
#0         DEPTH(m)    0.01      1.24     1.52      NaN
#1   BD 33kpa(t/m3)    1.60      1.60     1.60      NaN
#2          SAND(%)   42.10     42.10    65.10      NaN
#3          SILT(%)   37.90     37.90    16.90      NaN
#4          CLAY(%)   20.00     20.00    18.00      NaN
#5          ROCK(%)   12.00     12.00    12.00      NaN
#6       WLS(kg/ha)    0.00      5.00     0.10      5.1
#7       WLM(kg/ha)    0.00      5.00     0.10      5.1
#8      WLSL(kg/ha)    0.00      4.00     0.10      4.1
#9      WLSC(kg/ha)    0.00      2.10     0.00      2.1
#10     WLMC(kg/ha)    0.00      2.10     0.00      2.1
#11    WLSLC(kg/ha)    0.00      1.70     0.00      1.7
#12   WLSLNC(kg/ha)    0.00      0.40     0.00      0.4
#13     WBMC(kg/ha)    9.00   1102.10   250.90   1361.9
#14     WHSC(kg/ha)   69.00   8432.00  1920.00  10420.0
#15     WHPC(kg/ha)  146.00  18018.00  4102.00  22266.0
#16      WOC(kg/ha)  224.00  27556.00  6272.00     34.0
#17     WLSN(kg/ha)    0.00      0.00     0.00      0.0
#18     WLMN(kg/ha)    0.00      0.20     0.00      0.2
#19     WBMN(kg/ha)    0.90    110.20    25.10    136.2
#20     WHSN(kg/ha)    7.00    843.00   192.00   1042.0
#21     WHPN(kg/ha)   15.00   1802.00   410.00   2227.0
#22      WON(kg/ha)   22.00   2755.00   627.00   3405.0
#23     CFEM(kg/ha)    0.00       NaN      NaN      NaN
  



  df = df.loc[:,'TOT']
print df
#0         NaN
#1         NaN
#2         NaN
#3         NaN
#4         NaN
#5         NaN
#6         5.1
#7         5.1
#8         4.1
#9         2.1
#10        2.1
#11        1.7
#12        0.4
#13     1361.9
#14    10420.0
#15    22266.0
#16       34.0
#17        0.0
#18        0.2
#19      136.2
#20     1042.0
#21     2227.0
#22     3405.0
#23        NaN
  

 EDIT: 
If you have no repeat data in file and white-spaces between columns are more as one space (then separator is  \s\s+ ): 

  import pandas as pd

#parse data to dataframe df
#sep - http://stackoverflow.com/a/1546245/2901002
df = pd.read_table('test/file.txt', 
                   sep = '\s\s+', 
                   skiprows = 3,
                   skip_footer = 1,
                   header=None,
                   index_col=[0],
                   engine = 'python',
                   names=['i','1','2','3','TOT']
                    )
print df
df = df['TOT']
print df
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/50804035)
 there's a situation where the cell has white space, you can't see it, use  

  df['col'].replace('  ', np.nan, inplace=True)
  

 to replace white space as NaN, 

 then  

  df= df.dropna(subset=['col'])
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/42462626)
 I think you could also just opt for http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html.  

  df.replace(' ', '_', regex=True)
  

  Outputs  

        Person_1    Person_2     Person_3
0   John_Smith  Jane_Smith   Mark_Smith
1  Harry_Jones  Mary_Jones  Susan_Jones
  

 

 From some rough benchmarking, it predictably seems like piRSquared's NumPy solution is indeed the fastest, for this small sample at least, followed by  DataFrame.replace .  

  %timeit df.values[:] = np.core.defchararray.replace(df.values.astype(str), ' ', '_')
10000 loops, best of 3: <b>78.4 µs per loop</b>

%timeit df.replace(' ', '_', regex=True)
1000 loops, best of 3: <b>932 µs per loop</b>

%timeit df.stack().str.replace(' ', '_').unstack()
100 loops, best of 3: <b>2.29 ms per loop</b>
  

  Interestingly  however, it appears that piRSquared's Pandas solution scales  much  better with larger DataFrames than  DataFrame.replace , and even outperforms the NumPy solution.  

  >>> df = pd.DataFrame([['John Smith', 'Jane Smith', 'Mark Smith']*10000,
                       ['Harry Jones', 'Mary Jones', 'Susan Jones']*10000])
  

  %timeit df.values[:] = np.core.defchararray.replace(df.values.astype(str), ' ', '_')
10 loops, best of 3: <b>181 ms per loop</b>

%timeit df.replace(' ', '_', regex=True)
1 loop, best of 3: <b>4.14 s per loop</b>

%timeit df.stack().str.replace(' ', '_').unstack()
10 loops, best of 3: <b>99.2 ms per loop</b>
  



