Query: Fastest way to sort each row in a pandas dataframe
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/35731606)
 You could use pd.apply. 

  Eg:

A = pd.DataFrame(np.random.randint(0,100,(4,5)), columns=['one','two','three','four','five']) 
print (A)

   one  two  three  four  five
0    2   75     44    53    46
1   18   51     73    80    66
2   35   91     86    44    25
3   60   97     57    33    79

A = A.apply(np.sort, axis = 1) 
print(A)

   one  two  three  four  five
0    2   44     46    53    75
1   18   51     66    73    80
2   25   35     44    86    91
3   33   57     60    79    97
  

 Since you want it in descending order, you can simply multiply the dataframe with -1 and sort it. 

  A = pd.DataFrame(np.random.randint(0,100,(4,5)), columns=['one','two','three','four','five'])
A = A * -1
A = A.apply(np.sort, axis = 1)
A = A * -1
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/25818117)
 I think I would do this in numpy: 

  In [11]: a = df.values

In [12]: a.sort(axis=1)  # no ascending argument

In [13]: a = a[:, ::-1]  # so reverse

In [14]: a
Out[14]:
array([[8, 4, 3, 1],
       [9, 7, 2, 2]])

In [15]: pd.DataFrame(a, df.index, df.columns)
Out[15]:
   A  B  C  D
0  8  4  3  1
1  9  7  2  2
  

 

 I had thought this might work, but it sorts the columns: 

  In [21]: df.sort(axis=1, ascending=False)
Out[21]:
   D  C  B  A
0  1  8  4  3
1  2  7  2  9
  

 Ah, pandas raises: 

  In [22]: df.sort(df.columns, axis=1, ascending=False)
  

 
   ValueError: When sorting by column, axis must be 0 (rows) 
 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/50091705)
 Probably not the fastest, but this works: 

  rmask = df.c == -1
cmask = ['d', 'e', 'f', 'g']
df.loc[rmask, cmask] = df.loc[rmask, cmask].apply(lambda row: sorted(row), axis=1)
df
   a  b  c  d  e  f  g
0  x  y  1  3  4  5  6
1  x  y -1  5  6  7  8
2  x  y -1  3  4  7  8
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/29841807)
 To Add to the answer given by @Andy-Hayden, to do this inplace to the whole frame... not really sure why this works, but it does. There seems to be no control on the order. 

      In [97]: A = pd.DataFrame(np.random.randint(0,100,(4,5)), columns=['one','two','three','four','five'])

    In [98]: A
    Out[98]: 
    one  two  three  four  five
    0   22   63     72    46    49
    1   43   30     69    33    25
    2   93   24     21    56    39
    3    3   57     52    11    74

    In [99]: A.values.sort
    Out[99]: <function ndarray.sort>

    In [100]: A
    Out[100]: 
    one  two  three  four  five
    0   22   63     72    46    49
    1   43   30     69    33    25
    2   93   24     21    56    39
    3    3   57     52    11    74

    In [101]: A.values.sort()

    In [102]: A
    Out[102]: 
    one  two  three  four  five
    0   22   46     49    63    72
    1   25   30     33    43    69
    2   21   24     39    56    93
    3    3   11     52    57    74
    In [103]: A = A.iloc[:,::-1]

    In [104]: A
    Out[104]: 
    five  four  three  two  one
    0    72    63     49   46   22
    1    69    43     33   30   25
    2    93    56     39   24   21
    3    74    57     52   11    3
  

 I hope someone can explain the why of this, just happy that it works 8) 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/50389889)
 The fastest option depends not only on length of the DataFrame (in this case, around 13M rows) but also on the number of groups. Below are perfplots which compare a number of ways of finding the maximum in each group: 

 If there an only a  few (large) groups ,  using_idxmax  may be the fastest option:
https://i.stack.imgur.com/0TrV3.png 

 If there are  many (small) groups and the DataFrame is not too large ,  using_sort_drop  may be the fastest option:
<a href="https://i.stack.imgur.com/LTopt.png"  

 Keep in mind, however, that while  using_sort_drop ,  using_sort  and  using_rank  start out looking very fast, as  N = len(df)  increases, their speed relative to the other options disappears quickly.  For large enough  N ,  using_idxmax  becomes the fastest option , even if there are many groups. 

  using_sort_drop ,  using_sort  and  using_rank  sorts the DataFrame (or groups within the DataFrame). Sorting is  O(N * log(N))  on average, while the other methods use  O(N)  operations. This is why methods like  using_idxmax  beats  using_sort_drop  for very large DataFrames. 

 Be aware that benchmark results may vary for a number of reasons, including machine specs, OS, and software versions. So it is important to run benchmarks on your own machine, and with test data tailored to your situation. 

 Based on the perfplots above,  using_sort_drop   may be  an option worth considering for your DataFrame of 13M rows, especially if it has many (small) groups. Otherwise, I would suspect  using_idxmax  to be the fastest option -- but again, it's important that you check benchmarks on your machine. 

 

 Here is the setup I used to make the https://github.com/nschloe/perfplot: 

  import numpy as np
import pandas as pd 
import perfplot

def make_df(N):
    # lots of small groups
    df = pd.DataFrame(np.random.randint(N//10+1, size=(N, 2)), columns=['Id','delta'])
    # few large groups
    # df = pd.DataFrame(np.random.randint(10, size=(N, 2)), columns=['Id','delta'])
    return df


def using_idxmax(df):
    return df.loc[df.groupby("Id")['delta'].idxmax()]

def max_mask(s):
    i = np.asarray(s).argmax()
    result = [False]*len(s)
    result[i] = True
    return result

def using_custom_mask(df):
    mask = df.groupby("Id")['delta'].transform(max_mask)
    return df.loc[mask]

def using_isin(df):
    idx = df.groupby("Id")['delta'].idxmax()
    mask = df.index.isin(idx)
    return df.loc[mask]

def using_sort(df):
    df = df.sort_values(by=['delta'], ascending=False, kind='mergesort')
    return df.groupby('Id', as_index=False).first()

def using_rank(df):
    mask = (df.groupby('Id')['delta'].rank(method='first', ascending=False) == 1)
    return df.loc[mask]

def using_sort_drop(df):
    # Thanks to jezrael
    # https://stackoverflow.com/questions/50381064/select-the-max-row-per-group-pandas-performance-issue/50389889?noredirect=1#comment87795818_50389889
    return df.sort_values(by=['delta'], ascending=False, kind='mergesort').drop_duplicates('Id')

def using_apply(df):
    selected_idx = df.groupby("Id").apply(lambda df: df.delta.argmax())
    return df.loc[selected_idx]

def check(df1, df2):
    df1 = df1.sort_values(by=['Id','delta'], kind='mergesort').reset_index(drop=True)
    df2 = df2.sort_values(by=['Id','delta'], kind='mergesort').reset_index(drop=True)
    return df1.equals(df2)

perfplot.show(
    setup=make_df,
    kernels=[using_idxmax, using_custom_mask, using_isin, using_sort, 
             using_rank, using_apply, using_sort_drop],
    n_range=[2**k for k in range(2, 20)],
    logx=True,
    logy=True,
    xlabel='len(df)',
    repeat=75,
    equality_check=check)
  

 

 Another way to benchmark is to use https://stackoverflow.com/a/29280612/190597:  

  In [55]:  df = make_df(2**20)

In [56]: %timeit using_sort_drop(df)
1 loop, best of 3: 403 ms per loop

In [57]: %timeit using_rank(df)
1 loop, best of 3: 1.04 s per loop

In [58]: %timeit using_idxmax(df)
1 loop, best of 3: 15.8 s per loop
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/46668028)
 You can use list comprehension in http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html object by index -  level=0 ,  sort=False  change default sorting for faster solution: 

  L = [x for i, x in df.groupby(level=0, sort=False)]
  

 

  np.random.seed(123)
N = 1000
L = list('abcdefghijklmno')
df = pd.DataFrame({'A': np.random.choice(L, N),
                   'B':np.random.randint(10, size=N)}, index=np.random.randint(100, size=N))

In [273]: %timeit [x for i, x in df.groupby(level=0, sort=False)]
100 loops, best of 3: 9.91 ms per loop

In [274]: %timeit [df.loc[x] for x in df.index]
1 loop, best of 3: 417 ms per loop
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/43384414)
 You can use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html or http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex_axis.html, what is faster as  loc : 

 For  index : 

  idx = ['z','x','y']
df = df.reindex(idx)
print (df)
A  a  b  c  d
B            
z  6  3  8  1
x  6  7  6  7
y  5  6  5  3
  

  

  idx = ['z','x','y']
df = df.reindex_axis(idx)
print (df)
A  a  b  c  d
B            
z  6  3  8  1
x  6  7  6  7
y  5  6  5  3
  

 As https://stackoverflow.com/questions/43384397/pandas-is-there-a-native-way-to-sort-rows-by-providing-a-list-of-index-labels/43384414#comment73830523_43384397 pointed: 

  df = df.loc[['z', 'x', 'y'], :]
print (df)
A  a  b  c  d
B            
z  6  3  8  1
x  6  7  6  7
y  5  6  5  3
  

  

  cols = ['d','a','b','c']
df = df.reindex(columns=cols)
print (df)
A  d  a  b  c
B            
x  7  6  7  6
y  3  5  6  5
z  1  6  3  8

cols = ['d','a','b','c']
df = df.reindex_axis(cols, axis=1)
print (df)
A  d  a  b  c
B            
x  7  6  7  6
y  3  5  6  5
z  1  6  3  8
  

 

 Both: 

  idx = ['z','x','y']
cols = ['d','a','b','c']
df = df.reindex(columns=cols, index=idx)
print (df)
A  d  a  b  c
B            
z  1  6  3  8
x  7  6  7  6
y  3  5  6  5
  

  Timings : 

  In [43]: %timeit (df.loc[['z', 'x', 'y'], ['d', 'a', 'b', 'c']])
1000 loops, best of 3: 653 µs per loop

In [44]: %timeit (df.reindex(columns=cols, index=idx))
1000 loops, best of 3: 402 µs per loop
  

  

  In [49]: %timeit (df.reindex(idx))
The slowest run took 5.16 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 271 µs per loop

In [50]: %timeit (df.reindex_axis(idx))
The slowest run took 6.50 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 252 µs per loop


In [51]: %timeit (df.loc[['z', 'x', 'y']])
The slowest run took 5.51 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 418 µs per loop

In [52]: %timeit (df.loc[['z', 'x', 'y'], :])
The slowest run took 4.87 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 542 µs per loop
  

 

  def pir(df):
    idx = ['z','x','y']
    a = df.index.values.searchsorted(idx)
    df = pd.DataFrame(
        df.values[a],
        df.index[a], df.columns
    )
    return df

In [63]: %timeit (pir(df))
The slowest run took 7.75 times longer than the fastest. This could mean that an intermediate result is being cached.
10000 loops, best of 3: 91.8 µs per loop
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/50035453)
 In pandas is best avoid  loops s -  iterrows  and  apply  (loops under hood), better are vectorized solutions. 

 Use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html with parameter  on : 

  #for improve performance sort index and columns
df2 = df2.sort_index()
df1 = df1.sort_values(['key1','key2'])

df = df1.join(df2, on=['key1','key2'])
print (df)
   key1  key2  val    c
0     1   100  NaN    a
1     2   500  NaN  NaN
2     4   400  NaN    e
  

 EDIT: 

 Another aproach is join  MultiIndex  and columns values and use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html: 

  df2.index = ['{}_{}'.format(a,b) for a, b in df2.index]
print (df2)
       c
1_100  a
2_200  b
3_300  j
4_400  e
5_500  t

df1['joined'] = df1['key1'].astype(str) + '_' + df1['key2'].astype(str)
print (df1)
   key1  key2  val joined
0     1   100  NaN  1_100
1     2   500  NaN  2_500
2     4   400  NaN  4_400

df1['col'] = df1['joined'].map(df2['c'])
print (df1)
   key1  key2  val joined  col
0     1   100  NaN  1_100    a
1     2   500  NaN  2_500  NaN
2     4   400  NaN  4_400    e
  

  Timings : 

  np.random.seed(123)
N = 100000
df2 = pd.DataFrame(np.random.randint(10000, size=(N, 3)), columns=list('abc'))
df2 = df2.drop_duplicates(['a','b']).set_index(['a','b'])
print (df2.head())
              c
a    b         
3582 1346  5218
7763 9785  7382
5857 96    6257
6782 4143  4169
5664 942   6368

df1 = df2.iloc[np.random.randint(N, size=10)].reset_index()
df1.columns = ['key1','key2','val']
print (df1)
   key1  key2   val
0  5157  9207   283
1  6452  6474  7092
2  1264  5009  5123
3    86  7225  1025
4  7787  5134   637
5  9406  6119  8719
6  7479  1493  1525
7  4098  7248  7618
8  9921  7925  8547
9  2320   764  1564
  

 1.Join with unsorted  MultiIndex , columns: 

  In [42]: %timeit df1.join(df2, on=['key1','key2'])
100 loops, best of 3: 11.1 ms per loop
  

 2.Then first sort and then join (sorting is not used in timings): 

  df2 = df2.sort_index()

In [44]: %timeit df1.join(df2, on=['key1','key2'])
100 loops, best of 3: 10.5 ms per loop
  

 3. map  solution, also join  MultiIndex  is not count in timings, if still same data only run once: 

  df2.index = ['{}_{}'.format(a,b) for a, b in df2.index]
df1['joined'] = df1['key1'].astype(str) + '_' + df1['key2'].astype(str)

In [51]: %timeit df1['col'] = df1['joined'].map(df2['c'])
1000 loops, best of 3: 371 µs per loop
  

 

  In [55]: %%timeit
    ...: df1['joined'] = df1['key1'].astype(str) + '_' + df1['key2'].astype(str)
    ...: df1['col'] = df1['joined'].map(df2['c'])
    ...: 
1000 loops, best of 3: 1.08 ms per loop
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/53180806)
 Almost certainly your bottleneck is from https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html: 

  alldata_gaps = alldata_gaps.append(alldata.iloc[i])
alldata_gaps = alldata_gaps.append(Series)
  

 As an aside, you've confusingly named a variable the same as a Pandas object  pd.Series . It's good practice to avoid such ambiguity. 

 A  much  more efficient solution is to: 

 
 Identify times after which gaps occur. 
 Create a single dataframe with data for these times + 3 seconds. 
 Append to your existing dataframe and sort by time. 
 

 So let's have a stab with a sample dataframe: 

  # example dataframe setup
df = pd.DataFrame({'Date': ['00:10:15', '00:15:20', '00:15:40', '00:16:50', '00:17:55',
                            '00:19:00', '00:19:10', '00:19:15', '00:19:55', '00:20:58'],
                   'Value': list(range(10))})

df['Date'] = pd.to_datetime('2018-11-06-' + df['Date'])

# find gaps greater than 1 minute
bools = (df['Date'].diff().dt.seconds > 60).shift(-1).fillna(False)
idx = bools[bools].index
# Int64Index([0, 2, 3, 4, 8], dtype='int64')

# construct dataframe to append
df_extra = df.loc[idx].copy().assign(Value=np.nan)

# add 3 seconds
df_extra['Date'] = df_extra['Date'] + pd.to_timedelta('3 seconds')

# append to original
res = df.append(df_extra).sort_values('Date')
  

  

  print(res)

                 Date  Value
0 2018-11-06 00:10:15    0.0
0 2018-11-06 00:10:18    NaN
1 2018-11-06 00:15:20    1.0
2 2018-11-06 00:15:40    2.0
2 2018-11-06 00:15:43    NaN
3 2018-11-06 00:16:50    3.0
3 2018-11-06 00:16:53    NaN
4 2018-11-06 00:17:55    4.0
4 2018-11-06 00:17:58    NaN
5 2018-11-06 00:19:00    5.0
6 2018-11-06 00:19:10    6.0
7 2018-11-06 00:19:15    7.0
8 2018-11-06 00:19:55    8.0
8 2018-11-06 00:19:58    NaN
9 2018-11-06 00:20:58    9.0
  



