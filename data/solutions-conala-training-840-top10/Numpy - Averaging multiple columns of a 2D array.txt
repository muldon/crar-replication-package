Query: Numpy - Averaging multiple columns of a 2D array
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/14401218)
  data.reshape(-1,j).mean(axis=1).reshape(data.shape[0],-1)
  

 If your  j  divides  data.shape[1] , that is. 

 Example: 

  In [40]: data
Out[40]: 
array([[7, 9, 7, 2],
       [7, 6, 1, 5],
       [8, 1, 0, 7],
       [8, 3, 3, 2]])

In [41]: data.reshape(-1,j).mean(axis=1).reshape(data.shape[0],-1)
Out[41]: 
array([[ 8. ,  4.5],
       [ 6.5,  3. ],
       [ 4.5,  3.5],
       [ 5.5,  2.5]])
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/14401230)
 First of all, it looks to me like you're not averaging the columns at all, you're just averaging two data points at a time.  Seems to me like your best off http://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html the array, so your that you have a Nx2 data structure that you can feed directly to  mean .  You may have to pad it first if the number of columns isn't quite compatible.  Then at the end, just do a weighted average of the padded remainder column and the one before it.  Finally reshape back to the shape you want. 

 To play off of the example provided by TheodrosZelleke: 

  In [1]: data = np.concatenate((data, np.array([[5, 6, 7, 8]]).T), 1)

In [2]: data
Out[2]: 
array([[7, 9, 7, 2, 5],
       [7, 6, 1, 5, 6],
       [8, 1, 0, 7, 7],
       [8, 3, 3, 2, 8]])

In [3]: cols = data.shape[1]

In [4]: j = 2

In [5]: dataPadded = np.concatenate((data, np.zeros((data.shape[0], j - cols % j))), 1)

In [6]: dataPadded
Out[6]: 
array([[ 7.,  9.,  7.,  2.,  5.,  0.],
       [ 7.,  6.,  1.,  5.,  6.,  0.],
       [ 8.,  1.,  0.,  7.,  7.,  0.],
       [ 8.,  3.,  3.,  2.,  8.,  0.]])

In [7]: dataAvg = dataPadded.reshape((-1,j)).mean(axis=1).reshape((data.shape[0], -1))

In [8]: dataAvg
Out[8]: 
array([[ 8. ,  4.5,  2.5],
       [ 6.5,  3. ,  3. ],
       [ 4.5,  3.5,  3.5],
       [ 5.5,  2.5,  4. ]])

In [9]: if cols % j:
    dataAvg[:, -2] = (dataAvg[:, -2] * j + dataAvg[:, -1] * (cols % j)) / (j + cols % j)
    dataAvg = dataAvg[:, :-1]
   ....:     

In [10]: dataAvg
Out[10]: 
array([[ 8.        ,  3.83333333],
       [ 6.5       ,  3.        ],
       [ 4.5       ,  3.5       ],
       [ 5.5       ,  3.        ]])
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/8090605)
 Here's an example based on https://stackoverflow.com/questions/4624112/grouping-2d-numpy-array-in-average/4624923#4624923 (for clarity): 



<pre class="lang-py prettyprint-override"> >>> import numpy as np
>>> a = np.arange(24).reshape((4,6))
>>> a
array([[ 0,  1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10, 11],
       [12, 13, 14, 15, 16, 17],
       [18, 19, 20, 21, 22, 23]])
>>> a.reshape((2,a.shape[0]//2,3,-1)).mean(axis=3).mean(1)
array([[  3.5,   5.5,   7.5],
       [ 15.5,  17.5,  19.5]])
  

 As a function: 

<pre class="lang-py prettyprint-override"> def rebin(a, shape):
    sh = shape[0],a.shape[0]//shape[0],shape[1],a.shape[1]//shape[1]
    return a.reshape(sh).mean(-1).mean(1)
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/29042041)
 J.F. Sebastian has a great answer for 2D binning.  Here is a version of his "rebin" function that works for N dimensions: 

  def bin_ndarray(ndarray, new_shape, operation='sum'):
    """
    Bins an ndarray in all axes based on the target shape, by summing or
        averaging.

    Number of output dimensions must match number of input dimensions and 
        new axes must divide old ones.

    Example
    -------
    >>> m = np.arange(0,100,1).reshape((10,10))
    >>> n = bin_ndarray(m, new_shape=(5,5), operation='sum')
    >>> print(n)

    [[ 22  30  38  46  54]
     [102 110 118 126 134]
     [182 190 198 206 214]
     [262 270 278 286 294]
     [342 350 358 366 374]]

    """
    operation = operation.lower()
    if not operation in ['sum', 'mean']:
        raise ValueError("Operation not supported.")
    if ndarray.ndim != len(new_shape):
        raise ValueError("Shape mismatch: {} -> {}".format(ndarray.shape,
                                                           new_shape))
    compression_pairs = [(d, c//d) for d,c in zip(new_shape,
                                                  ndarray.shape)]
    flattened = [l for p in compression_pairs for l in p]
    ndarray = ndarray.reshape(flattened)
    for i in range(len(new_shape)):
        op = getattr(ndarray, operation)
        ndarray = op(-1*(i+1))
    return ndarray
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/26931969)
 You can transform them into a 3-D masked array and then take the average along  axis=0 , for example: 

  np.ma.array((a, b, c)).mean(axis=0)
  

 Example: 

  import numpy as np

a = np.ma.array([[99, 99, 99],
                 [ 1.,  2., 99],
                 [ 2.,  3., 99],
                 [99, 99, 99]],
                 mask=[[True, True, True],
                       [False, False, True],
                       [False, False, True],
                       [True, True, True]])
b = np.ma.array([[99,  2.,  2.],
                 [99,  2.,  3.],
                 [99, 99, 99],
                 [99, 99, 99]],
                 mask=[[True, False, False],
                       [True, False, False],
                       [True, True, True],
                       [True, True, True]])
c = np.ma.array([[2., 1., 99],
                 [1., 1., 99],
                 [99, 99, 99],
                 [99, 99, 99]],
                 mask=[[False, False, True],
                       [False, False, True],
                       [True, True, True],
                       [True, True, True]])
  

  

  np.ma.array((a, b, c)).mean(axis=0)

masked_array(data =
 [[2.0 1.5 2.0]
 [1.0 1.6666666666666667 3.0]
 [2.0 3.0 --]
 [-- -- --]],
             mask =
 [[False False False]
 [False False False]
 [False False  True]
 [ True  True  True]],
       fill_value = 1e+20)
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/18461943)
 You can create a 3D array containing your 2D arrays to be averaged, then average along  axis=0  using  np.mean  or  np.average  (the latter allows for weighted averages): 

  np.mean( np.array([ old_set, new_set ]), axis=0 )
  

 This averaging scheme can be applied to any  (n) -dimensional array, because the created  (n+1) -dimensional array will always contain the original arrays to be averaged along its  axis=0 . 


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/29436890)
 Without  ski-learn , you can simply reshape, and take the appropriate mean. 

  M=np.arange(10000).reshape(100,100)
M1=M.reshape(10,10,10,10)
M2=M1.mean(axis=(1,3))
  

 quick check to see if I got the right axes 

  In [127]: M2[0,0]
Out[127]: 454.5

In [128]: M[:10,:10].mean()
Out[128]: 454.5

In [131]: M[-10:,-10:].mean()
Out[131]: 9544.5

In [132]: M2[-1,-1]
Out[132]: 9544.5
  

 Adding  .transpose([0,2,1,3])  puts the 2 averaging dimensions at the end, as  view_as_blocks  does. 

 For this  (100,100)  case, the reshape approach is 2x faster than the  as_strided  approach, but both are quite fast. 

 However the direct strided solution isn't much slower than reshaping. 

  as_strided(M,shape=(10,10,10,10),strides=(8000,80,800,8)).mean((2,3))
as_strided(M,shape=(10,10,10,10),strides=(8000,800,80,8)).mean((1,3))
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/29435971)
 You could split your array into blocks with the http://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.view_as_blocks function (in scikit-image). 

 For a 2D array, this returns a 4D array with the blocks ordered row-wise: 

  >>> import skimage.util as ski
>>> import numpy as np
>>> a = np.arange(16).reshape(4,4) # 4x4 array
>>> ski.view_as_blocks(a, (2,2))
array([[[[ 0,  1],
         [ 4,  5]],

        [[ 2,  3],
         [ 6,  7]]],


       [[[ 8,  9],
         [12, 13]],

        [[10, 11],
         [14, 15]]]])
  

 Taking the mean along the last two axes returns a 2D array with the mean in each block: 

  >>> ski.view_as_blocks(a, (2,2)).mean(axis=(2,3))
array([[  2.5,   4.5],
       [ 10.5,  12.5]])
  

  Note :  view_as_blocks  returns a view of the array by modifying the strides (it also works with arrays with more than two dimensions). It is implemented purely in NumPy using  as_strided , so if you don't have access to the scikit-image library you can https://github.com/scikit-image/scikit-image/blob/master/skimage/util/shape.py#l8. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/34245504)
 I ended up writing a small function that upscales the image using  scipy.ndimage.zoom , but for downscaling it first upscales it to be the multiple of the original shape, and then downscales by block-averaging. It accepts any other keyword arguments for  scipy.zoom  ( order  and  prefilter ) 

 I'm still looking for a cleaner solution using available packages.  

  def zoomArray(inArray, finalShape, sameSum=False, **zoomKwargs):
    inArray = np.asarray(inArray, dtype = np.double)
    inShape = inArray.shape
    assert len(inShape) == len(finalShape)
    mults = []
    for i in range(len(inShape)):
        if finalShape[i] < inShape[i]:
            mults.append(int(np.ceil(inShape[i]/finalShape[i])))
        else:
            mults.append(1)
    tempShape = tuple([i * j for i,j in zip(finalShape, mults)])

    zoomMultipliers = np.array(tempShape) / np.array(inShape) + 0.0000001
    rescaled = zoom(inArray, zoomMultipliers, **zoomKwargs)

    for ind, mult in enumerate(mults):
        if mult != 1:
            sh = list(rescaled.shape)
            assert sh[ind] % mult == 0
            newshape = sh[:ind] + [sh[ind] / mult, mult] + sh[ind+1:]
            rescaled.shape = newshape
            rescaled = np.mean(rescaled, axis = ind+1)
    assert rescaled.shape == finalShape

    if sameSum:
        extraSize = np.prod(finalShape) / np.prod(inShape)
        rescaled /= extraSize
    return rescaled
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/36224760)
 I was trying to downscale a raster -- take a roughly 6000 by 2000 size raster and turn it into an arbitrarily sized smaller raster that averaged the values properly across the previous bins sizes. I found a solution using SciPy, but then I couldn't get SciPy to install on the shared hosting service I was using, so I just wrote this function instead. There is likely a better ways to do this that doesn't involve looping through the rows and columns, but this does seem to work.  

 The nice part about this is that the old number of rows and columns don't have to be divisible by the new number of rows and columns. 

  def resize_array(a, new_rows, new_cols): 
    '''
    This function takes an 2D numpy array a and produces a smaller array 
    of size new_rows, new_cols. new_rows and new_cols must be less than 
    or equal to the number of rows and columns in a.
    '''
    rows = len(a)
    cols = len(a[0])
    yscale = float(rows) / new_rows 
    xscale = float(cols) / new_cols

    # first average across the cols to shorten rows    
    new_a = np.zeros((rows, new_cols)) 
    for j in range(new_cols):
        # get the indices of the original array we are going to average across
        the_x_range = (j*xscale, (j+1)*xscale)
        firstx = int(the_x_range[0])
        lastx = int(the_x_range[1])
        # figure out the portion of the first and last index that overlap
        # with the new index, and thus the portion of those cells that 
        # we need to include in our average
        x0_scale = 1 - (the_x_range[0]-int(the_x_range[0]))
        xEnd_scale =  (the_x_range[1]-int(the_x_range[1]))
        # scale_line is a 1d array that corresponds to the portion of each old
        # index in the_x_range that should be included in the new average
        scale_line = np.ones((lastx-firstx+1))
        scale_line[0] = x0_scale
        scale_line[-1] = xEnd_scale
        # Make sure you don't screw up and include an index that is too large
        # for the array. This isn't great, as there could be some floating
        # point errors that mess up this comparison.
        if scale_line[-1] == 0:
            scale_line = scale_line[:-1]
            lastx = lastx - 1
        # Now it's linear algebra time. Take the dot product of a slice of
        # the original array and the scale_line
        new_a[:,j] = np.dot(a[:,firstx:lastx+1], scale_line)/scale_line.sum()

    # Then average across the rows to shorten the cols. Same method as above.
    # It is probably possible to simplify this code, as this is more or less
    # the same procedure as the block of code above, but transposed.
    # Here I'm reusing the variable a. Sorry if that's confusing.
    a = np.zeros((new_rows, new_cols))
    for i in range(new_rows):
        the_y_range = (i*yscale, (i+1)*yscale)
        firsty = int(the_y_range[0])
        lasty = int(the_y_range[1])
        y0_scale = 1 - (the_y_range[0]-int(the_y_range[0]))
        yEnd_scale =  (the_y_range[1]-int(the_y_range[1]))
        scale_line = np.ones((lasty-firsty+1))
        scale_line[0] = y0_scale
        scale_line[-1] = yEnd_scale
        if scale_line[-1] == 0:
            scale_line = scale_line[:-1]
            lasty = lasty - 1
        a[i:,] = np.dot(scale_line, new_a[firsty:lasty+1,])/scale_line.sum() 

    return a 
  



