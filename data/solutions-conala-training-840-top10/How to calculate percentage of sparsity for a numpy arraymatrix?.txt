Query: How to calculate percentage of sparsity for a numpy array/matrix?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/54442410)
  Definition:  

 https://i.stack.imgur.com/RAlAt.png 

 Code for a general case: 

  from numpy import array
from numpy import count_nonzero
import numpy as np

# create dense matrix
A = array([[1, 1, 0, 1, 0, 0], [1, 0, 2, 0, 0, 1], [99, 0, 0, 2, 0, 0]])

#If you have Nan
A = np.nan_to_num(A,0)

# calculate sparsity
sparsity = 1.0 - ( count_nonzero(A) / float(A.size) )
print(sparsity)
  

  Results:  

  0.555555555556
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/38709042)
  np.isnan(a).sum()
  

 gives the number of  nan  values, in this example 8.   

  np.prod(a.shape)
  

 is the number of values, here 50.  Their ratio should give the desired value. 

  In [1081]: np.isnan(a).sum()/np.prod(a.shape)
Out[1081]: 0.16
  

 You might also find it useful to make a masked array from this 

  In [1085]: a_ma=np.ma.masked_invalid(a)
In [1086]: print(a_ma)
[[0.0 0.0 0.0 0.0 1.0]
 [1.0 1.0 0.0 -- --]
 [0.0 -- 1.0 -- --]
 [1.0 1.0 1.0 1.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 0.0 0.0 --]
 [-- -- 1.0 1.0 1.0]
 [0.0 1.0 0.0 1.0 0.0]
 [1.0 0.0 1.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]]
  

 The number of valid values then is: 

  In [1089]: a_ma.compressed().shape
Out[1089]: (42,)
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/20402224)
 First calculate the positions where  a  and  b  differ using  a != b , then find the mean of those values: 

  >>> import numpy as np
>>> a = np.array([1, 2, 3, 4, 5, 6, 7])
>>> b = np.array([1, 2, 3, 5, 5, 6, 7])
>>> error = np.mean( a != b )
>>> error
0.14285714285714285
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/38505354)
 The https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP package has a utility function for this, called count_table, which can be used to solve your problem efficiently as such: 

  import numpy_indexed as npi
arrs = [arr1, arr2, arr3]
idx = [np.ones(len(a))*i for i, a in enumerate(arrs)]
(rows, cols), table = npi.count_table(np.concatenate(idx), np.concatenate(arrs))
table = table / table.sum(axis=1, keepdims=True)
print(table * 100)
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/31040249)
 Scipy has inbuilt methods for sparse matrix multiplication. 

 Example from documentation: 

  >>> import numpy as np
>>> from scipy.sparse import csr_matrix
>>> Q = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])
>>> p = np.array([1, 0, -1])
>>> Q.dot(p)
array([ 1, -3, -1], dtype=int64)
  

 Check these resources: 

 http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csc_matrix.dot.html
http://docs.scipy.org/doc/scipy/reference/sparse.html 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/20402188)
  

  >>> a = np.array([1, 2, 3, 5, 5, 6, 7])
>>> b = np.array([1, 2, 3, 4, 5, 6, 7])
>>> (a != b).sum()/float(a.size)
0.14285714285714285
  

  Update  I'm courious why this one is  slightly  faster: 

  a = np.random.randint(4, size=1000)
b = np.random.randint(4, size=1000)
timeit('from __main__ import a, b; (a != b).sum()/float(a.size)', number=10000)
# 0.42409151163039496
timeit('from __main__ import a, b, np; np.mean(a != b)', number=10000)
# 0.5342614773662717
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/42310449)
  In [269]: from scipy import sparse
In [270]: M=sparse.random(1000,1000,.1, 'csr')
In [271]: MA = M.A
In [272]: timeit M*M.T
10 loops, best of 3: 64 ms per loop
In [273]: timeit MA@MA.T
10 loops, best of 3: 60.4 ms per loop
  

 I defined a random sparse matrix with a specified sparsity, 10%: 

  In [274]: M
Out[274]: 
<1000x1000 sparse matrix of type '<class 'numpy.float64'>'
    with 100000 stored elements in Compressed Sparse Row format>
In [275]: np.allclose(MA@MA.T, (M*M.T).A)
Out[275]: True
  

  @  is an operator form of  dot  (see  np.matmul ).  So at this 10% level of sparsity, the two approaches time the same (without any conversion to/from sparse). 

 For this random matrix, the  M*M.T  result is dense: 

  In [282]: (M*M.T)
Out[282]: 
<1000x1000 sparse matrix of type '<class 'numpy.float64'>'
    with 999964 stored elements in Compressed Sparse Row format>
  

 

 The sparse times depend heavily on sparsity; the dense times not at all 

  In [295]: M=sparse.random(1000,1000,.01, 'csr'); MA=M.A
In [296]: timeit M*M.T
100 loops, best of 3: 2.44 ms per loop
In [297]: timeit MA@MA.T
10 loops, best of 3: 56.3 ms per loop
In [298]: M=sparse.random(1000,1000,.2, 'csr'); MA=M.A
In [299]: timeit M*M.T
10 loops, best of 3: 175 ms per loop
In [300]: timeit MA@MA.T
10 loops, best of 3: 56.3 ms per loop
  

 

 With a round trip to sparse and back, times jump from 60 to 100ms 

  In [302]: %%timeit
     ...: M1=sparse.csr_matrix(MA)
     ...: (M1*M1.T).A
     ...: 
10 loops, best of 3: 104 ms per loop
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/37780423)
 Hi you can do it this way 

      temp = sp.coo_matrix((data, (row, col)), shape=(3, 59))
    temp1 = temp.tocsr()

    #Cosine similarity
    row_sums = ((temp1.multiply(temp1)).sum(axis=1))
    rows_sums_sqrt = np.array(np.sqrt(row_sums))[:,0]
    row_indices, col_indices = temp1.nonzero()
    temp1.data /= rows_sums_sqrt[row_indices]
    temp2 = temp1.transpose()
    temp3 = temp1*temp2
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/39104306)
 You can compute pairwise cosine similarity on the rows of a sparse matrix directly using sklearn.  As of version 0.17 it also supports sparse output: 

  from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

A =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])
A_sparse = sparse.csr_matrix(A)

similarities = cosine_similarity(A_sparse)
print('pairwise dense output:\n {}\n'.format(similarities))

#also can output sparse matrices
similarities_sparse = cosine_similarity(A_sparse,dense_output=False)
print('pairwise sparse output:\n {}\n'.format(similarities_sparse))
  

 Results: 

  pairwise dense output:
[[ 1.          0.40824829  0.40824829]
[ 0.40824829  1.          0.33333333]
[ 0.40824829  0.33333333  1.        ]]

pairwise sparse output:
(0, 1)  0.408248290464
(0, 2)  0.408248290464
(0, 0)  1.0
(1, 0)  0.408248290464
(1, 2)  0.333333333333
(1, 1)  1.0
(2, 1)  0.333333333333
(2, 0)  0.408248290464
(2, 2)  1.0
  

 If you want column-wise cosine similarities simply transpose your input matrix beforehand:  

  A_sparse.transpose()
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/53468487)
 Measuring the percentage of missing values has already explained by 'hpaulj'. 

 I am taking the first part of your question, Assuming array has Zero's and Non-Zero's... 

 Sparsity refers to Zero values and density refers to Non-Zero values in array.
Suppose your array is X, 
get count of non-zero values: 

  non_zero = np.count_nonzero(X)  

 total values in X: 

  total_val = np.product(X.shape)  

 Sparsity will be - 

  sparsity = (total_val - non_zero) / total_val  

 And Density will be - 

  density = non_zero / total_val  

 The sum of Sparsity and Density must equal to 100%... 



