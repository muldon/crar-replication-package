Query: http file downloading and saving
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/25827767)
 I use https://pypi.python.org/pypi/wget. 

 Simple and good library if you want to example? 

  import wget

file_url = 'http://johndoe.com/download.zip'

file_name = wget.download(file_url)
  

<p module support python 2 and python 3 versions 


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/19602990)
 A clean way to download a file is: 

  import urllib

testfile = urllib.URLopener()
testfile.retrieve("http://randomsite.com/file.gz", "file.gz")
  

 This downloads a file from a website and names it  file.gz . This is one of my favorite solutions, from https://stackoverflow.com/questions/3042757/downloading-a-picture-via-urllib-and-python. 

 This example uses the  urllib  library, and it  will directly retrieve the file form a source. 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/19603003)
 As mentioned https://stackoverflow.com/questions/22676/how-do-i-download-a-file-over-http-using-python/22776#22776: 

  import urllib
urllib.urlretrieve ("http://randomsite.com/file.gz", "file.gz")
  

  EDIT:  If you still want to use requests, take a look at https://stackoverflow.com/questions/14114729/save-a-file-using-the-python-requests-library or https://stackoverflow.com/questions/13137817/how-to-download-image-using-requests. 


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/1731369)
 Yes, it will fetch the file. 

 I think what you really want to do is send a HTTP HEAD request (which basically asks the server not for the data itself, but for the headers only). you can look https://stackoverflow.com/questions/107405/how-do-you-send-a-head-http-request-in-python. 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/45279314)
 Four methods using wget, urllib and request. 

  #!/usr/bin/python
import requests
from StringIO import StringIO
from PIL import Image
import profile as profile
import urllib
import wget


url = 'https://tinypng.com/images/social/website.jpg'

def testRequest():
    image_name = 'test1.jpg'
    r = requests.get(url, stream=True)
    with open(image_name, 'wb') as f:
        for chunk in r.iter_content():
            f.write(chunk)

def testRequest2():
    image_name = 'test2.jpg'
    r = requests.get(url)
    i = Image.open(StringIO(r.content))
    i.save(image_name)

def testUrllib():
    image_name = 'test3.jpg'
    testfile = urllib.URLopener()
    testfile.retrieve(url, image_name)

def testwget():
    image_name = 'test4.jpg'
    wget.download(url, image_name)

if __name__ == '__main__':
    profile.run('testRequest()')
    profile.run('testRequest2()')
    profile.run('testUrllib()')
    profile.run('testwget()')
  

 testRequest  - 4469882 function calls (4469842 primitive calls) in 20.236 seconds 

 testRequest2 - 8580 function calls (8574 primitive calls) in 0.072 seconds 

 testUrllib   - 3810 function calls (3775 primitive calls) in 0.036 seconds 

 testwget     - 3489 function calls in 0.020 seconds 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/1731388)
 You want to actually tell the server  not  to send the full content of the file. HTTP has a mechanism for this called "HEAD" that is an alternative to "GET". It works the same way, but the server only sends you the headers, none of the actual content. 

 That'll save at least one of you bandwidth, while simply not doing a read() will only not bother getting the full file. 

  

  import httplib
c = httplib.HTTPConnection(<hostname>)
c.request("HEAD", <url>)
print c.getresponse().status
  

 The status code will be printed. Url should only be a segment, like "/foo" and hostname should be like, "www.example.com". 


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/8814861)
 You don't need to use  zipfile.ZipFile  for this (and the way you're using it, as well as  urllib2.urlopen , has problems as well). Instead, you need to save the urlopen result in a variable, then  read  it and write that output to a .zip file. Try this code: 

  #download file
download = "http://status.calibre-ebook.com/dist/portable/" + result
request = urllib2.urlopen( download )

#save
output = open("install.zip", "w")
output.write(request.read())
output.close()
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/10472524)
 I suggest installing pip, an alternative to easy_install, first.  

 I am not sure what tools OSX comes with for downloading the bootstrap code for pip (curl, wget), but I assume you have python 2.X installed, which you can check with  

  python --version
  

 Run the following python program, by saving this as download.py and then run 'python download.py': 

  import os
import urllib2
from subprocess import call

def download_file_and_run(url):
    basename = os.path.basename(url)
    fp = urllib2.urlopen(url)
    open(basename, 'wb').write(fp.read())
    fp.close()
    call(['python', basename])

download_file_and_run('http://python-distribute.org/distribute_setup.py')
download_file_and_run('https://raw.github.com/pypa/pip/master/contrib/get-pip.py')
  

  

  pip --version 
  

 should give you a version number like 1.1 

 Now install rpy2 using: 

  pip install rpy2
  

 This assumes R is installed, if not the installer will complain with an error:
Tried to guess R's HOME but no R command in the PATH. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/8814863)
 If you just want to download a file from the net, you can use http://docs.python.org/library/urllib.html#urllib.urlretrieve: 

 
   Copy a network object denoted by a URL to a local file ... 
 

 Example using http://docs.python-requests.org/ instead of  urllib2 : 

  import requests, re, urllib

print("Calibre is updating...")
content = requests.get("http://sourceforge.net/projects/calibre/files").content

# determine current version
v = re.search('title="/[0-9.]*/([a-zA-Z\-]*-[0-9\.]*)', content).groups()[0][:-1]
download_url = "http://status.calibre-ebook.com/dist/portable/{0}".format(v)

print("Downloading {0}".format(download_url))
urllib.urlretrieve(download_url, 'install.zip')
# file should be downloaded at this point
  



