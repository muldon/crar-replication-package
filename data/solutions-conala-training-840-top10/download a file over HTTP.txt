Query: download a file over HTTP
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/12100154)
 You can use urllib.urlretrieve (in Python 3.x: urllib.request.urlretrieve): 

  import urllib
urllib.urlretrieve('http://site.com/', filename='filez.txt')
  

 This should be work :) 

 and this is a fnction that can do the same thing (using urllib): 

  def download(url):
    webFile = urllib.urlopen(url)
    localFile = open(url.split('/')[-1], 'w')
    localFile.write(webFile.read())
    webFile.close()
    localFile.close()
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/18100004)
  

  bytes=5000-
  

 so you instruct HTTP server to start FROM specified location and send you everything after specified offset 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/20218209)
 Source code can be: 

  import urllib
sock = urllib.urlopen("http://diveintopython.org/")
htmlSource = sock.read()                            
sock.close()                                        
print htmlSource  
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/33543751)
 I use threading module for download threads:
Also requests, but you can change that to urllib by yourself. 

  import threading
import requests



def download(link, filelocation):
    r = requests.get(link, stream=True)
    with open(filelocation, 'wb') as f:
        for chunk in r.iter_content(1024):
            if chunk:
                f.write(chunk)

def createNewDownloadThread(link, filelocation):
    download_thread = threading.Thread(target=download, args=(link,filelocation))
    download_thread.start()

for i in range(0,5):
    file = "C:\\test" + str(i) + ".png"
    print file
    createNewDownloadThread("http://stackoverflow.com/users/flair/2374517.png", file)
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/1798945)
 It is possible to do partial download using the range header, the following will request a selected range of bytes: 

  req = urllib2.Request('http://www.python.org/')
req.headers['Range'] = 'bytes=%s-%s' % (start, end)
f = urllib2.urlopen(req)
  

 For example: 

  >>> req = urllib2.Request('http://www.python.org/')
>>> req.headers['Range'] = 'bytes=%s-%s' % (100, 150)
>>> f = urllib2.urlopen(req)
>>> f.read()
'l1-transitional.dtd">\n\n\n<html xmlns="http://www.w3.'
  

 Using this header you can resume partial downloads. In your case all you have to do is to keep track of already downloaded size and request a new range. 

 Keep in mind that the server need to accept this header for this to work. 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/1798984)
 This is quite easy to do using TCP sockets and raw HTTP. The relevant request header is "Range". 

 An example request might look like: 

  mysock = connect(("www.example.com", 80))
mysock.write(
  "GET /huge-growing-file HTTP/1.1\r\n"+\
  "Host: www.example.com\r\n"+\
  "Range: bytes=XXXX-\r\n"+\
  "Connection: close\r\n\r\n")
  

 Where XXXX represents the number of bytes you've already retrieved. Then you can read the response headers and any content from the server. If the server returns a header like: 

  Content-Length: 0
  

 You know you've got the entire file. 

 If you want to be particularly nice as an HTTP client you can look into "Connection: keep-alive". Perhaps there is a python library that does everything I have described (perhaps even urllib2 does it!) . 


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/16259955)
 This solution requires the linux utility named "aria2c", but it has the advantage of easily resuming downloads. 

 It also assumes that all the files you want to download are listed in the http directory list for location  MY_HTTP_LOC .  I tested this script on an instance of lighttpd/1.4.26 http server. But, you can easily modify this script so that it works for other setups. 

  #!/usr/bin/python

import os
import urllib
import re
import subprocess

MY_HTTP_LOC = "http://AAA.BBB.CCC.DDD/"

# retrieve webpage source code
f = urllib.urlopen(MY_HTTP_LOC)
page = f.read()
f.close

# extract relevant URL segments from source code
rgxp = '(\<td\ class="n"\>\<a\ href=")([0-9a-zA-Z\(\)\-\_\.]+)(")'
results =  re.findall(rgxp,str(page))
files = []
for match in results:
    files.append(match[1])

# download (using aria2c) files
for afile in files:
    if os.path.exists(afile) and not os.path.exists(afile+'.aria2'):
        print 'Skipping already-retrieved file: ' + afile
    else:
        print 'Downloading file: ' + afile          
        subprocess.Popen(["aria2c", "-x", "16", "-s", "20", MY_HTTP_LOC+str(afile)]).wait()
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/48691447)
 In python3 you can use urllib3 and shutil libraires.
Download them by using pip or pip3 (Depending whether python3 is default or not) 

  pip3 install urllib3 shutil
  

 Then run this code 

  import urllib.request
import shutil

url = "http://www.somewebsite.com/something.pdf"
output_file = "save_this_name.pdf"
with urllib.request.urlopen(url) as response, open(output_file, 'wb') as out_file:
    shutil.copyfileobj(response, out_file)
  

 Note that you download  urllib3  but use  urllib  in code 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/6373260)
 How about http://docs.python.org/library/urllib.html#urllib.urlretrieve 

  import urllib
urllib.urlretrieve('http://python.org/images/python-logo.gif', '/tmp/foo.gif')
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/45541027)
 You can download file like this 

  import urllib2
response = urllib2.urlopen('http://www.example.com/file_to_download')
html = response.read()
  

 To get all the links in a page 

  from bs4 import BeautifulSoup

import requests
r  = requests.get("http://site-to.crawl")
data = r.text
soup = BeautifulSoup(data)

for link in soup.find_all('a'):
    print(link.get('href'))
  



