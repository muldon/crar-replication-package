Query: How to filter rows in pandas by regex
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/15333283)
 Use https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html instead: 

  In [10]: df.b.str.contains('^f')
Out[10]: 
0    False
1     True
2     True
3    False
Name: b, dtype: bool
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/48884429)
 Write a Boolean function that checks the regex and use apply on the column 

  foo[foo['b'].apply(regex_function)]
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/54031740)
 Filter the DataFrame using a Boolean mask made from the given column and regex pattern as follows:
   df[df.column_name.str.contains('^[\d]*', regex=True)]  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/54031750)
 Use the http://pandas.pydata.g/pandas-docs/stable/basics.html#vectized-string-methods  contains()    match()  - see http://pandas.pydata.g/pandas-docs/stable/text.html#testing-f-strings-that-match--contain-a-pattern: 

  df[df.column_name.str.contains('^\d+')]
  

  

  df[df.column_name.str.match('\d+')]    # Matches only start of the string
  

 Note that I removed superfluous brackets ( [] ), and replaced  *  with  + , because the  \d*  will  always match  as it matches a  zero  occurrences, too (so called a  zero-length match .) 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/41829883)
 You can use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html to match each of the substrings by using the regex  |  character which implies an  OR  selection from the contents of the other series: 

  df[df['B'].str.contains("|".join(df['A']))]
  

 https://i.stack.imgur.com/AP6fn.png 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/33527315)
 use http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html#pandas.Series.str.contains and join the keys using  |  to create a regex pattern and negate the boolean mask  ~  to filter your df: 

  In [123]:
keys = ['key','key2']    â€‹
df[~df['Text'].str.contains('|'.join(keys))]

Out[123]:
   A              Text
0  5   Sample text one
1  6   Sample text two
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/52765987)
 Thanks for the great answer @user3136169, here is an example of how that might be done also removing NoneType values. 

  def regex_filter(val):
    if val:
        mo = re.search(regex,val)
        if mo:
            return True
        else:
            return False
    else:
        return False

df_filtered = df[df['col'].apply(regex_filter)]
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/35727164)
 So it looks like part of my problem with  filter  was that I was using an outdated version of pandas. After updating I no longer get the  TypeError . After some playing around, it looks like I can use  filter  to fit my needs. Here is what I found out. 

 Simply setting  df.filter(regex='string')  will return the columns which match the regex. This looks to do the same as  df.filter(regex='string', axis=1) . 

 To search the index, I simply need to do  df.filter(regex='string', axis=0)  



