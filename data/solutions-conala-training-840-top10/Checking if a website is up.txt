Query: Checking if a website is up
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/38145622)
  from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
req = Request("http://stackoverflow.com")
try:
    response = urlopen(req)
except HTTPError as e:
    print('The server couldn\'t fulfill the request.')
    print('Error code: ', e.code)
except URLError as e:
    print('We failed to reach a server.')
    print('Reason: ', e.reason)
else:
    print ('Website is working fine')
  

 Works on Python 3 


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/1949393)
 You can use http://docs.python.org/library/httplib.html 

  import httplib
conn = httplib.HTTPConnection("www.python.org")
conn.request("HEAD", "/")
r1 = conn.getresponse()
print r1.status, r1.reason
  

  

  200 OK
  

 Of course, only if  www.python.org  is up. 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/1949360)
 You could try to do this with  getcode()  from http://docs.python.org/library/urllib.html  

  >>> print urllib.urlopen("http://www.stackoverflow.com").getcode()
>>> 200
  

 EDIT: For more modern python, i.e.  python3 , use:  

  import urllib.request
print(urllib.request.urlopen("http://www.stackoverflow.com").getcode())
>>> 200
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/15743618)
 I think the easiest way to do it is by using http://docs.python-requests.org/en/latest/ module. 

  import requests

def url_ok(url):
    r = requests.head(url)
    return r.status_code == 200
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/46602823)
 If server if down, on python 2.7 x86 windows urllib have no timeout and program go to dead lock. So use urllib2 

  import urllib2
import socket

def check_url( url, timeout=5 ):
    try:
        return urllib2.urlopen(url,timeout=timeout).getcode() == 200
    except urllib2.URLError as e:
        return False
    except socket.timeout as e:
        print False


print check_url("http://google.fr")  #True 
print check_url("http://notexist.kc") #False     
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/45791021)
 Hi this class can do speed and up test for your web page with this class: 

   from urllib.request import urlopen
 from socket import socket
 import time


 def tcp_test(server_info):
     cpos = server_info.find(':')
     try:
         sock = socket()
         sock.connect((server_info[:cpos], int(server_info[cpos+1:])))
         sock.close
         return True
     except Exception as e:
         return False


 def http_test(server_info):
     try:
         # TODO : we can use this data after to find sub urls up or down    results
         startTime = time.time()
         data = urlopen(server_info).read()
         endTime = time.time()
         speed = endTime - startTime
         return {'status' : 'up', 'speed' : str(speed)}
     except Exception as e:
         return {'status' : 'down', 'speed' : str(-1)}


 def server_test(test_type, server_info):
     if test_type.lower() == 'tcp':
         return tcp_test(server_info)
     elif test_type.lower() == 'http':
         return http_test(server_info)
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/17414777)
 Python's Requests library is great for checking things like HTTP status codes (not downloading files, just getting the response) 

 For example: 

  import requests

r = requests.get('website.com/file_2013-06-27.zip')
if r.status_code == 200:
    print ("File uploaded.")
  

 That doesn't download the file (just tried it with a 1GB file), just checks if the web server will serve it and what the HTTP response is. With HTTP, 200 means that the file exists, and is accessible. See below for more info on HTTP response codes. 

 
   More info:  
  http://docs.python-requests.org/en/latest/ - the requests library 
  http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html - guide to HTTP response codes 
 


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/7482086)
 You can test if the website is up or down with a simple http://docs.python.org/library/httplib.html?highlight=httpconnection#httplib.HTTPConnection, send a "OPTIONS *" request, if the answer is "200 OK", your site is up, otherwise, check the http error codes. 

  import httplib
connection = httplib.HTTPConnection(your_host, port_probably_80)
connection.request("OPTIONS", "*")
response = connection.getresponse()
if response.status != httplib.OK or response.reason != "OK":
   print "Down"
else:
   print "Up :)"
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/38321251)
 Seems like the site is checking to see where you're mouse is before passing you to the next page. Moving to the element before clicking it works for me: 

  driver = webdriver.Chrome()
driver.get('http://119.254.209.77/')
time.sleep(5)
pageSource = driver.page_source
print(driver.page_source)
# the target url
checking = driver.find_element_by_id('_ctl0__ctl0_Content_MenuHyperLink2')

action_chain = webdriver.ActionChains(driver)
action_chain.move_to_element(checking)
action_chain.click(checking)
action_chain.perform()
time.sleep(2)
print(driver.page_source)
  



