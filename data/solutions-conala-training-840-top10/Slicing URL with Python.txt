Query: Slicing URL with Python
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/258800)
 The quick and dirty solution is this: 

  >>> "http://something.com/page?CONTENT_ITEM_ID=1234&param3".split("&")[0]
'http://something.com/page?CONTENT_ITEM_ID=1234'
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/258810)
 Another option would be to use the split function, with & as a parameter. That way, you'd extract both the base url and both parameters. 

     url.split("&") 
  

 returns a list with 

    ['http://www.domainname.com/page?CONTENT_ITEM_ID=1234', 'param2', 'param3']
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/258797)
 I figured it out below is what I needed to do: 

  url = "http://www.domainname.com/page?CONTENT_ITEM_ID=1234&param2&param3"
url = url[: url.find("&")]
print url
'http://www.domainname.com/page?CONTENT_ITEM_ID=1234'
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/30650849)
 You can use a combination of https://docs.python.org/2/library/urlparse.html and https://docs.python.org/2/library/stdtypes.html#str.split. 

  import urlparse
url = "http://127.0.0.1:8000/data/number/"
path = urlparse.urlparse(url).path
val = path.split("/")[2]
print val
  

 This prints: 

  number
  

 

 The output of  urlparse  for the above URL is  

  ParseResult(scheme='http', netloc='127.0.0.1:8000', path='/data/number/', params='', query='', fragment='')
  

 We are utilizing the  path  portion of this tuple. We split it on  /  and take the second index.  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/259054)
 Parsin URL is never as simple I it seems to be, that's why there are the urlparse and urllib modules. 

 E.G : 

  import urllib
url ="http://www.domainname.com/page?CONTENT_ITEM_ID=1234&param2&param3"
query = urllib.splitquery(url)
result = "?".join((query[0], query[1].split("&")[0]))
print result
'http://www.domainname.com/page?CONTENT_ITEM_ID=1234'
  

 This is still not 100 % reliable, but much more than splitting it yourself because there are a lot of valid url format that you and me don't know and discover one day in error logs. 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/30650844)
 use urlparse module life would be easier 

  from urlparse import urlparse
urlparse('http://127.0.0.1:8000/data/number/').path.split('/')[2]
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/50667123)
 Here's one way which uses a list of strings and http://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.apply.html for finding a valid URL: 

  s = pd.Series(['tweet text goes here https://example.com',
               'some http://other.com example',
               'www.thirdexample.com is here'])

test_strings = ['http', 'www']

def url_finder(x):
    return next(i for i in x.split() if any(t in i for t in test_strings))

res = s.apply(url_finder)

print(res)

0     https://example.com
1        http://other.com
2    www.thirdexample.com
dtype: object
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/6367294)
 Use http://docs.python.org/py3k/library/urllib.parse.html#urllib.parse.urlparse to carve the path from the rest of the url, and  posixpath.split()  and  posixpath.join()  to reform the path, and http://docs.python.org/py3k/library/urllib.parse.html#urllib.parse.urlunparse to put it all back together again. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/259159)
 Use the http://docs.python.org/2/library/urlparse.html#urlparse.urlsplit module.  

  import urlparse

def process_url(url, keep_params=('CONTENT_ITEM_ID=',)):
    parsed= urlparse.urlsplit(url)
    filtered_query= '&'.join(
        qry_item
        for qry_item in parsed.query.split('&')
        if qry_item.startswith(keep_params))
    return urlparse.urlunsplit(parsed[:3] + (filtered_query,) + parsed[4:])
  

 In your example: 

  >>> process_url(a)
'http://www.domainname.com/page?CONTENT_ITEM_ID=1234'
  

 This function has the added bonus that it's easier to use if you decide that you also want some more query parameters, or if the order of the parameters is not fixed, as in: 

  >>> url='http://www.domainname.com/page?other_value=xx&param3&CONTENT_ITEM_ID=1234&param1'
>>> process_url(url, ('CONTENT_ITEM_ID', 'other_value'))
'http://www.domainname.com/page?other_value=xx&CONTENT_ITEM_ID=1234'
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/15017516)
  

  from urlparse import urlparse as ue
urls = ['https://www.google.com', 'http://stackoverflow.com']
parsed = []
for url in urls:
    decoded = ue(url).hostname
    if decoded.startswith('www.'):
        decoded = ".".join(decoded.split('.')[1:])
    parsed.append(decoded.split('.')[0])
#parsed is now your parsed list of hostnames
  

 Also, you might want to change the if statement in the for loop, because some domains might start with other things that you would want to get rid of. 



