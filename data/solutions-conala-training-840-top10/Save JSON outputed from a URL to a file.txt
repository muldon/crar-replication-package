Query: Save JSON outputed from a URL to a file
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/4565038)
 You can use http://jackson.codehaus.org/: 

   ObjectMapper mapper = new ObjectMapper(); 
 Map<String,Object> map = mapper.readValue(url, Map.class);
 mapper.writeValue(new File("myfile.json"), map);
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/3040966)
 You can use http://en.wikipedia.org/wiki/CURL 

  curl -d "q=hi" http://search.twitter.com -o file1.txt
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/3040960)
 Here's the (verbose ;) ) Java variant: 

  InputStream input = null;
OutputStream output = null;
try {
    input = new URL("http://search.twitter.com/search.json?q=hi").openStream();
    output = new FileOutputStream("/output.json");
    byte[] buffer = new byte[1024];
    for (int length = 0; (length = input.read(buffer)) > 0;) {
        output.write(buffer, 0, length);
    }
    // Here you could append further stuff to `output` if necessary.
} finally {
    if (output != null) try { output.close(); } catch (IOException logOrIgnore) {}
    if (input != null) try { input.close(); } catch (IOException logOrIgnore) {}
}
  

  See also : 

 
 http://java.sun.com/docs/books/tutorial/essential/io/ 
 http://java.sun.com/docs/books/tutorial/networking/urls/readingWriting.html 
 https://stackoverflow.com/questions/2793150/how-to-use-java-net-urlconnection-to-fire-and-handle-http-requests/2793153#2793153 
 


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/47110106)
 There is no https://devdocs.io/python~3.6/library/urllib.request. You are maybe confusing the usage of http://docs.python-requests.org/en/master/ package with the inbuilt urllib library module. 

 

  >>> from urllib import request
>>> request.post
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: module 'urllib.request' has no attribute 'post'
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/3040917)
 This is easy in any language, but the mechanism varies.   

  wget 'http://search.twitter.com/search.json?q=hi' -O hi.json
  

 To append: 

  wget 'http://search.twitter.com/search.json?q=hi' -O - >> hi.json
  

  

  urllib.urlretrieve('http://search.twitter.com/search.json?q=hi', 'hi.json')
  

 To append: 

  hi_web = urllib2.urlopen('http://search.twitter.com/search.json?q=hi');
with open('hi.json', 'ab') as hi_file:
  hi_file.write(hi_web.read())
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/5184643)
 Here is another way of doing this with PHP and fOpen. 

  <?php
// Define your output file name and your search query
$output = 'result.txt';
$search = 'great';

write_twitter_to_file($output, $search);

/*
 * Writes Json responses from twitter API to a file output.
 * 
 * @param $output: The name of the file that contains the output 
 * @param $search: The search term query to use in the Twitter API
*/

function write_twitter_to_file($output, $search) {
    $search = urlencode($search);
    $url = 'http://search.twitter.com/search.json?q=' . $search;
    $handle = fopen($url, "r");

    if ($handle) {
        while (($buffer = fgets($handle, 4096)) !== false) {
            file_put_contents($output, $buffer, FILE_APPEND);
            echo "Output has been saved to file ";
        }

        if (!feof($handle)) {
            echo "Error: unexpected fgets() fail\n";
        }

        fclose($handle);
    }

}
?>
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/54382304)
 You can just iterate through each level of the dictionary and download the files if you find a url. 

  urls = []
for library in my_json['libraries']:
    for lib_name, lib_data in library.items():
        for module_name, module_data in lib_data.items():
            url = module_data.get('url')
            if url is not None:
                # create local directory with lib_name
                # download files from url to local directory
                urls.append(url)

# urls = ['foobar.com/.../library-1.bin', 'barfoo.com/.../library-2.exe']
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/16129667)
  import json
weather = urllib2.urlopen('url')
wjson = weather.read()
wjdata = json.loads(wjson)
print wjdata['data']['current_condition'][0]['temp_C']
  

 What you get from the url is a json string. And your can't parse it with index directly.
You should convert it to a dict by  json.loads  and then you can parse it with index. 

   .read()  to intermediately save it to memory and then read it to  json , allow  json  to load it directly from the file: 

  wjdata = json.load(urllib2.urlopen('url'))
  



