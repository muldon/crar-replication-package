Query: Scikit-learn: How to run KMeans on a one-dimensional array?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/28416647)
 You have many samples of 1 feature, so you can reshape the array to (13,876, 1) using numpy's http://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html: 

  from sklearn.cluster import KMeans
import numpy as np
x = np.random.random(13876)

km = KMeans()
km.fit(x.reshape(-1,1))  # -1 will be calculated to be 13876 here
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/42375180)
 Read about http://www.macwright.org/2013/02/18/literate-jenks.html. Function in Python found the link from the article: 

  def get_jenks_breaks(data_list, number_class):
    data_list.sort()
    mat1 = []
    for i in range(len(data_list) + 1):
        temp = []
        for j in range(number_class + 1):
            temp.append(0)
        mat1.append(temp)
    mat2 = []
    for i in range(len(data_list) + 1):
        temp = []
        for j in range(number_class + 1):
            temp.append(0)
        mat2.append(temp)
    for i in range(1, number_class + 1):
        mat1[1][i] = 1
        mat2[1][i] = 0
        for j in range(2, len(data_list) + 1):
            mat2[j][i] = float('inf')
    v = 0.0
    for l in range(2, len(data_list) + 1):
        s1 = 0.0
        s2 = 0.0
        w = 0.0
        for m in range(1, l + 1):
            i3 = l - m + 1
            val = float(data_list[i3 - 1])
            s2 += val * val
            s1 += val
            w += 1
            v = s2 - (s1 * s1) / w
            i4 = i3 - 1
            if i4 != 0:
                for j in range(2, number_class + 1):
                    if mat2[l][j] >= (v + mat2[i4][j - 1]):
                        mat1[l][j] = i3
                        mat2[l][j] = v + mat2[i4][j - 1]
        mat1[l][1] = 1
        mat2[l][1] = v
    k = len(data_list)
    kclass = []
    for i in range(number_class + 1):
        kclass.append(min(data_list))
    kclass[number_class] = float(data_list[len(data_list) - 1])
    count_num = number_class
    while count_num >= 2:  # print "rank = " + str(mat1[k][count_num])
        idx = int((mat1[k][count_num]) - 2)
        # print "val = " + str(data_list[idx])
        kclass[count_num - 1] = data_list[idx]
        k = int((mat1[k][count_num] - 1))
        count_num -= 1
    return kclass
  

 Use and visualization: 

  import numpy as np
import matplotlib.pyplot as plt

def get_jenks_breaks(...):...

x = np.random.random(30)
breaks = get_jenks_breaks(x, 5)

for line in breaks:
    plt.plot([line for _ in range(len(x))], 'k--')

plt.plot(x)
plt.grid(True)
plt.show()
  

 Result:
https://i.stack.imgur.com/li1hF.jpg 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/40370646)
 You shouldn't be refitting the vectorizer in the test stage, your code would be cleaner if you combine the vectorizer and classifier with a pipeline: 

  from sklearn.pipeline import make_pipeline
vectorizer = TfidfVectorizer(max_df=0.8, max_features=max_feat, norm="l1", analyzer="word",
                                 min_df=0.1,ngram_range=(1,2)
                                 )   
km = KMeans(n_clusters=number, init='k-means++', max_iter=100, n_init=3,
                    verbose=1, n_jobs = -2)
clf = make_pipeline(vectorizer, km)
clf.fit(X)


sample = df.tail(int(totalTestRows * lineLimit))

for row in sample.itertuples():
    test_data = np.array([row[6]])
    print(clf.predict(test_data))
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/32232618)
 For example 

  import numpy as np
from sklearn.cluster import KMeans
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

estimator = KMeans(n_clusters=3)
estimator.fit(X)
  

 You can get clusters of each point by 

  estimator.labels_
  

  

  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
   0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
   1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
   1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1,
   2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,
   1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1], dtype=int32)
  

 Then get the indices of points for each cluster 

  {i: np.where(estimator.labels_ == i)[0] for i in range(estimator.n_clusters)}
  

  

  {0: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,
        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),
 1: array([ 50,  51,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,
         64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,
         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,
         91,  92,  93,  94,  95,  96,  97,  98,  99, 101, 106, 113, 114,
        119, 121, 123, 126, 127, 133, 138, 142, 146, 149]),
 2: array([ 52,  77, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112,
        115, 116, 117, 118, 120, 122, 124, 125, 128, 129, 130, 131, 132,
        134, 135, 136, 137, 139, 140, 141, 143, 144, 145, 147, 148])}
  

  Edit  

 If you want to use array of points in  X  as values rather than the array of indices: 

  {i: X[np.where(estimator.labels_ == i)] for i in range(estimator.n_clusters)}
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/27491876)
 You can do it this way:  

  >>> import numpy as np
>>> import sklearn.cluster as cl
>>> data = np.array([99,1,2,103,44,63,56,110,89,7,12,37])
>>> k_means = cl.KMeans(init='k-means++', n_clusters=3, n_init=10)
>>> k_means.fit(data[:,np.newaxis]) # [:,np.newaxis] converts data from 1D to 2D
>>> k_means_labels = k_means.labels_
>>> k1,k2,k3 = [data[np.where(k_means_labels==i)] for i in range(3)] # range(3) because 3 clusters
>>> k1
array([44, 63, 56, 37])
>>> k2
array([ 99, 103, 110,  89])
>>> k3
array([ 1,  2,  7, 12])
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/28115682)
 You are passing a 1D array while scikit expects a 2D array with a  samples  and a  features  axis. This should do it: 

  k_means.fit(df.AgeFill.reshape(-1, 1))
  

  

  >>> df.AgeFill.shape
(891,)
  

  

  >>> df.AgeFill.reshape(-1, 1).shape
(891, 1)
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/23974300)
 If you haven't tried it yet, use http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans instead of  sklearn.cluster.KMeans  

 E.g., if  X.shape = (100000, 2048) , then write 

  from sklearn.cluster import MiniBatchKMeans
mbkm = MiniBatchKMeans(n_clusters=200)  # Take a good look at the docstring and set options here
mbkm.fit(X)
  

  MiniBatchKMeans  finds slightly different clusters from normal  KMeans , but has the huge advantage that it is an online algorithm which doesn't need all the data at every iteration and still gives useful results. 


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/20179838)
 A minor edit to answer your question about 2d:</h3>

 You can use the original answer below, just take: 

  data = np.column_stack([x,y])
  

 If you want to plot the centroids, it is the same as below in the original answer. If you want to color each value by the group selected, you can use  kmeans2  

  from scipy.cluster.vq import kmeans2

centroids, ks = kmeans2(data, 3, 10)
  

 To plot, pick  k  colors, then use the  ks  array returned by  kmeans2  to select that color from the three colors: 

  colors = ['r', 'g', 'b']
plt.scatter(*data.T, c=np.choose(ks, colors))
plt.scatter(*centroids.T, c=colors, marker='v')
  

   

 original answer:</h3>

 As @David points out, your  data  is one dimensional, so the centroid for each cluster will also just be one dimensional.  The reason your plot  looks  2d is because when you run 

  plt.plot(data)
  

 if  data  is 1d, then what the function actually does is plot: 

  plt.plot(range(len(data)), data)
  

 To make this clear, see this example: 

  data = np.array([3,2,3,4,3])
centroids, variances= kmeans(data, 3, 10)
plt.plot(data)
  

   

 Then the centroids will be one dimensional, so they have no  x  location in that plot, so you could plot them as lines, for example: 

  for c in centroids:
    plt.axhline(c)
  

   

 If you want to find the centroids of the x-y pairs where  x = range(len(data))  and  y = data , then you must pass those pairs to the clustering algorithm, like so: 

  xydata = np.column_stack([range(len(data)), data])
centroids, variances= kmeans(xydata, 3, 10)
  

 But I doubt this is what you want.  Probably, you want random  x   and   y  values, so try something like: 

  data = np.random.rand(100,2)
centroids, variances = kmeans(data, 3, 10)
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/41723989)
 To access the data points post k-means clustering :  

 added code :  

  sortedR = sorted(result, key=lambda x: x[1])
sortedR
  

 Complete code :  

      from sklearn.cluster import KMeans
    import numpy as np
    from matplotlib import pyplot

    X = np.array([[10, 2 , 9], [1, 4 , 3], [1, 0 , 3],
                   [4, 2 , 1], [4, 4 , 7], [4, 0 , 5], [4, 6 , 3],[4, 1 , 7],[5, 2 , 3],[6, 3 , 3],[7, 4 , 13]])
    kmeans = KMeans(n_clusters=3, random_state=0).fit(X)

    k = 3
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X)

    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

    for i in range(k):
        # select only data observations with cluster label == i
        ds = X[np.where(labels==i)]
        # plot the data observations
        pyplot.plot(ds[:,0],ds[:,1],'o')
        # plot the centroids
        lines = pyplot.plot(centroids[i,0],centroids[i,1],'kx')
        # make the centroid x's bigger
        pyplot.setp(lines,ms=15.0)
        pyplot.setp(lines,mew=2.0)
    pyplot.show()

result = zip(X , kmeans.labels_)

sortedR = sorted(result, key=lambda x: x[1])
sortedR
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/11537071)
 This is already pre-computed at  fit  time in the  inertia_  attribute for the  KMeans  class. 

  >>> from sklearn.datasets import load_iris
>>> from sklearn.cluster import KMeans
>>> iris = load_iris()
>>> km = KMeans(3).fit(iris.data)
>>> km.inertia_
78.940841426146108
  



