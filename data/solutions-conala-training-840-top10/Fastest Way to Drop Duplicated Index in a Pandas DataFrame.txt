Query: Fastest Way to Drop Duplicated Index in a Pandas DataFrame
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/33387356)
 The 'duplicated' method works for dataframes and for series. Just select on those rows which aren't marked as having a duplicate index: 

  df[~df.index.duplicated()]
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/22918691)
 Simply:  DF.groupby(DF.index).  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/22918493)
 You can use  numpy.unique  to obtain the index of unique values and use  iloc  to get those indices: 

  >>> df
        val
A  0.021372
B  1.229482
D -1.571025
D -0.110083
C  0.547076
B -0.824754
A -1.378705
B -0.234095
C -1.559653
B -0.531421

[10 rows x 1 columns]

>>> idx = np.unique(df.index, return_index=True)[1]
>>> df.iloc[idx]
        val
A  0.021372
B  1.229482
C  0.547076
D -1.571025

[4 rows x 1 columns]
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/52105205)
 Call  reset_index  and  duplicated , and then index the original: 

  df = df[~df.reset_index().duplicated().values]
print (df)
   A  B
a  0  1
b  0  0
c  0  0
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/47534117)
 I think you need convert  index  to  Series  by http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.to_series.html,  keep='first'  should be omit, because default parameter in http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.duplicated.html: 

  df = df[~df.index.to_series().duplicated()]
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/53395360)
 Using https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html (or  loc , see below) and https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.drop.html: 

  df = pd.DataFrame(np.arange(0, 1000000, 1))
indices = np.arange(0, 1000000, 3)

%timeit -n 100 df[~df.index.isin(indices)]
%timeit -n 100 df.iloc[df.index.drop(indices)]

41.3 ms ± 997 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
32.7 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
  

 As @jezrael points out you can only use  iloc  if  index  is a https://pandas.pydata.org/pandas-docs/stable/generated/pandas.RangeIndex.html otherwise you will have to use  loc . But this is still faster than  df[df.isin()]  (see why below). 

 All three options on 10 million rows: 

  df = pd.DataFrame(np.arange(0, 10000000, 1))
indices = np.arange(0, 10000000, 3)

%timeit -n 10 df[~df.index.isin(indices)]
%timeit -n 10 df.iloc[df.index.drop(indices)]
%timeit -n 10 df.loc[df.index.drop(indices)]

4.98 s ± 76.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
752 ms ± 51.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
2.65 s ± 69.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
  

 

  Why does super slow  loc  outperform  boolean_indexing ?  

 Well, the short answer is that it doesn't.  df.index.drop(indices)  is just a lot faster than  ~df.index.isin(indices)  (given above data with 10 million rows): 

  %timeit -n 10 ~df.index.isin(indices)
%timeit -n 10 df.index.drop(indices)

4.55 s ± 129 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
388 ms ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
  

 We can compare this to the performance of  boolean_indexing  vs  iloc  vs  loc : 

  boolean_mask = ~df.index.isin(indices)
dropped_index = df.index.drop(indices)

%timeit -n 10 df[boolean_mask]
%timeit -n 10 df.iloc[dropped_index]
%timeit -n 10 df.loc[dropped_index]


489 ms ± 25.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
371 ms ± 10.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
2.38 s ± 153 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/44393212)
 Let's use the information Jezrael provided. 

 Input Dataframe: 

  print(df)
           Stock  Open  High   Low  Close  Adj Close  Volume
2016-05-13   AAD  5.23  5.26  5.20   5.26       5.26    5000
2016-05-16   AAD  5.22  5.26  5.22   5.26       5.26    6000
2016-05-17   AAD  5.21  5.26  5.21   5.26       5.26    2000
2016-05-17   AAD  5.21  5.26  5.21   5.26       5.26    2000
2016-05-18   AAD  5.20  5.25  5.20   5.25       5.25    3000

df1 = df[~df.index.duplicated(keep='last')]
print(df1)
  

 Output: 

             Stock  Open  High   Low  Close  Adj Close  Volume
2016-05-13   AAD  5.23  5.26  5.20   5.26       5.26    5000
2016-05-16   AAD  5.22  5.26  5.22   5.26       5.26    6000
2016-05-17   AAD  5.21  5.26  5.21   5.26       5.26    2000
2016-05-18   AAD  5.20  5.25  5.20   5.25       5.25    3000
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/53397341)
 If order of rows doesn't mind, you can arrange them in place : 

  n=10**7
df=pd.DataFrame(arange(4*n).reshape(n,4))
indices=np.unique(randint(0,n,size=n//2))

from numba import njit
@njit
def _dropfew(values,indices):
    k=len(values)-1
    for ind in indices[::-1]:
            values[ind]=values[k]
            k-=1

def dropfew(df,indices):
    _dropfew(df.values,indices)
    return df.iloc[:len(df)-len(indices)]
  

  

  In [39]: %time df.iloc[df.index.drop(indices)]
Wall time: 1.07 s

In [40]: %time dropfew(df,indices)
Wall time: 219 ms
  


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/36923497)
 Setup</h3>

  from StringIO import StringIO
import pandas as pd

text="""    source  target  value   type
0   10  1200    0.500   Undirected
1   13  3333    0.600   Undirected
2   10  1200    0.500   Undirected
3   15  2300    0.350   Undirected
4   18  5300    0.250   Undirected
5   17  2300    0.100   Undirected
6   13  3333    0.600   Undirected"""

df = pd.read_csv(StringIO(text), delim_whitespace=True, index_col=[0])
  

 Solution</h3>

  print df[df.duplicated()]

   source  target  value        type
2      10    1200    0.5  Undirected
6      13    3333    0.6  Undirected

print df.drop_duplicates(keep=False)

   source  target  value        type
3      15    2300   0.35  Undirected
4      18    5300   0.25  Undirected
5      17    2300   0.10  Undirected
  

 Explanation</h3>

  df.duplicated()  returns boolean mask on what was duplicated 

  df.drop_duplicates()  drops the duplicated rows 

  keep=False  specifies to drop all rows that have been duplicated as opposed to keeping the first or last of the duplicated rows. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html 


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/48166341)
 Use sorting( np.sort ) and then get duplicates( .duplicated() ) out of it.
Later use that duplicates to drop( df.drop ) the required index 

  import pandas as pd
import numpy as np
df = pd.DataFrame(columns=['a','b','c','d'], index=['1','2','3'])
df.loc['1'] = pd.Series({'a':'x','b':'y','c':'e','d':'f'})
df.loc['2'] = pd.Series({'a':'e','b':'f','c':'x','d':'y'})
df.loc['3'] = pd.Series({'a':'w','b':'v','c':'s','d':'t'})

df_duplicated = pd.DataFrame(np.sort(df, axis=1), index=df.index).duplicated()
index_to_drop = [ind for ind in range(len(df_duplicated)) if df_duplicated[ind]]
df.drop(df.index[df_duplicated])
  



