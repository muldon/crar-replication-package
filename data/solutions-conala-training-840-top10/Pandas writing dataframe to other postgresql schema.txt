Query: Pandas writing dataframe to other postgresql schema
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/24191432)
 Update: starting from pandas 0.15, writing to different schema's is supported. Then you will be able to use the  schema  keyword argument: 

  df.to_sql('test', engine, schema='a_schema')
  

 

 Writing to different schema's is not yet supported at the moment with the  read_sql  and  to_sql  functions (but an enhancement request has already been filed: https://github.com/pydata/pandas/issues/7441). 

 However, you can get around for now using the object interface with  PandasSQLAlchemy  and providing a custom  MetaData  object: 

  meta = sqlalchemy.MetaData(engine, schema='a_schema')
meta.reflect()
pdsql = pd.io.sql.PandasSQLAlchemy(engine, meta=meta)
pdsql.to_sql(df, 'test')
  

  This interface ( PandasSQLAlchemy ) is not yet really public and will still undergo changes in the next version of pandas, but this is how you can do it for pandas 0.14. 

  Update :  PandasSQLAlchemy  is renamed to  SQLDatabase  in pandas 0.15. 


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/24204524)
 Solved, thanks to joris answer. 
Code was also improved thanks to joris comment, by passing around sqlalchemy engine instead of connection objects. 

  import pandas as pd
from sqlalchemy import create_engine, MetaData

engine = create_engine(r'postgresql://some:user@host/db')
meta = sqlalchemy.MetaData(engine, schema='a_schema')
meta.reflect(engine, schema='a_schema')
pdsql = pd.io.sql.PandasSQLAlchemy(engine, meta=meta)

df = pd.read_sql("SELECT * FROM xxx", con=engine)    
pdsql.to_sql(df, 'test')
  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/18718081)
 @javax's answer is almost correct; the following is a little clarification: 

  q = exists(select([("schema_name")]).select_from("information_schema.schemata")
    .where("schema_name = 'foo'"))
if not session.query(q).scalar():
    session.execute('CREATE SCHEMA foo;')
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/51294670)
 I believe you create a connection to a postgreSQL database using SQLAlchemy and then pass that connection to the  con  kwarg. For example: 

  import numpy as np
import pandas as pd
import sqlalchemy

dates = pd.date_range('20130101',periods=6)
df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))

url = 'postgresql://USER:PASSWORD@HOST:PORT/DATABASE'
con = sqlalchemy.create_engine(url, client_encoding='utf8')
print(pd.io.sql.get_schema(df.reset_index(), 'data', con=con))
CREATE TABLE data (
        index TIMESTAMP WITHOUT TIME ZONE,
        "A" FLOAT(53),
        "B" FLOAT(53),
        "C" FLOAT(53),
        "D" FLOAT(53)
)
  


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/18347034)
 If you want to integrate it with SQLAlchemy you could use http://docs.sqlalchemy.org/en/rel_0_8/core/schema.html#metadata-reflection but for an easier and quicker solution: 

  from sqlalchemy.sql import exists, select
exists(select([("schema_name")]).select_from("information_schema.schemata").
       where("schema_name == 'foo'"))
  

 This will return  True  or  False . 


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/24263165)
 Since pandas 0.14, the sql functions also support postgresql (via SQLAlchemy, so all database flavors supported by SQLAlchemy work). So you can simply use  to_sql  to write a pandas DataFrame to a PostgreSQL database: 

  import pandas as pd
from sqlalchemy import create_engine
import psycopg2
engine = create_engine('postgresql://scott:tiger@localhost:5432/mydatabase')

df.to_sql("table_name", engine)
  

 See the docs: http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries 

 If you have an older version of pandas (< 0.14), see this question:https://stackoverflow.com/questions/23103962/how-to-write-dataframe-to-postgres-table/23104436#23104436 


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/41741132)
 I would manually truncate the table and then simply let Pandas do its job: 

  con.execute('TRUNCATE my_table RESTART IDENTITY;')
df.to_sql('my_table', con, if_exists='append')
  


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/25176837)
  Update : starting from pandas 0.15, writing to different schema's is supported. Then you will be able to use the  schema  keyword argument: 

  df.to_sql('test', engine, schema='a_schema')
  

 

 As I said in the linked https://stackoverflow.com/questions/24189150/pandas-writing-dataframe-to-other-postgresql-schema/24191432#24191432, writing to different schema's is not yet supported at the moment with the  read_sql  and  to_sql  functions (but an enhancement request has already been filed: https://github.com/pydata/pandas/issues/7441). 

 However, I described a workaround using the object interface. But what I described there only works for adding the table  once , not for replacing and/or appending the table. So if you just want to add, first delete the existing table and then write again.   

 If you want to append to the table, below is a little bit more hacky workaround. First redefine  has_table  and  get_table : 

  def has_table(self, name):
    return self.engine.has_table(name, schema=self.meta.schema)

def get_table(self, table_name):
    if self.meta.schema:
        table_name = self.meta.schema + '.' + table_name
    return self.meta.tables.get(table_name)

pd.io.sql.PandasSQLAlchemy.has_table = has_table
pd.io.sql.PandasSQLAlchemy.get_table = get_table
  

 Then create the  PandasSQLAlchemy  object as you did, and write the data: 

  meta = sqlalchemy.MetaData(engine, schema='schema')
meta.reflect()
pdsql = pd.io.sql.PandasSQLAlchemy(engine, meta=meta)
pdsql.to_sql(df, 'table', if_exists='append')
  

 This is obviously not the good way to do, but we are working to provide a better API for 0.15. If you want to help, pitch in at https://github.com/pydata/pandas/issues/7441. 

  This interface ( PandasSQLAlchemy ) is not yet really public and will still undergo changes in the next version of pandas, but this is how you can do it for pandas 0.14(.1).  

  Update :  PandasSQLAlchemy  is renamed to  SQLDatabase  in pandas 0.15. 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/48307108)
 Try to specify a schema name: 

  result.to_sql('ds_attribution_probabilities', con=engine, 
              schema='online', index=False, if_exists='append')
  


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/26595613)
 I suppose you are using pandas 0.15.  PandasSQLAlchemy  was not yet really public, and was renamed in pandas 0.15 to  SQLDatabase . So if you replace that in your code, it should work (so  pdsql = pd.io.sql.SQLDatabase(engine, meta=meta) ). 

 However, starting from pandas 0.15, there is also schema support in the  read_sql_table  and  to_sql  functions, so it should not be needed to make a  MetaData  and  SQLDatabase  object manually. Instead, this should do it: 

  dataframe.to_sql(table_name, engine, schema='data_quality')
  

 See the 0.15 release notes: http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#improvements-in-the-sql-io-module 



