Query: How to check if all values in the columns of a numpy matrix are the same?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/46770701)
  

  df = pd.DataFrame([[1,2],[3,4]],columns=['A','B'])
my_arr = np.array([[2,2],[2,2]])

df += my_arr

print(df)
  

 Output: 

     A  B
0  3  4
1  5  6
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/51304317)
 This might be more efficient than nested loops: 

  out = []
for row in x:
    idx = np.equal(n[:,:2], row).all(1)
    out.extend(n[idx,2].tolist())
  

 Note this assumes that  x  is of shape  (?, 2) . Otherwise, if it has more than two columns, just change  row  to  row[:2]  in the loop body.  


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/28777437)
 https://github.com/pydata/pandas/pull/6132 there's http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.strings.StringMethods.get_dummies.html: 

  In [11]: s = pd.Series(["3 4", "2 3 4 5"])

In [12]: s.str.get_dummies(sep=" ")
Out[12]:
   2  3  4  5
0  0  1  1  0
1  1  1  1  1
  

 You have to ensure the columns are integers (rather than strings) and reindex: 

  In [13]: df = s.str.get_dummies(sep=" ")

In [14]: df.columns = df.columns.map(int)

In [15]: df.reindex(columns=np.arange(6), fill_value=0)
Out[15]:
   0  1  2  3  4  5
0  0  0  0  1  1  0
1  0  0  1  1  1  1
  

 To get the numpy values use  .values : 

  In [16]: df.reindex(columns=np.arange(6), fill_value=0).values
Out[16]:
array([[0, 0, 0, 1, 1, 0],
       [0, 0, 1, 1, 1, 1]])
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/14860884)
  In [45]: a
Out[45]: 
array([[1, 1, 0],
       [1, 0, 0],
       [1, 0, 0],
       [1, 1, 0]])
  

 Compare each value to the corresponding value in the first row: 

  In [46]: a == a[0,:]
Out[46]: 
array([[ True,  True,  True],
       [ True, False,  True],
       [ True, False,  True],
       [ True,  True,  True]], dtype=bool)
  

 A column shares a common value if all the values in that column are True: 

  In [47]: np.all(a == a[0,:], axis = 0)
Out[47]: array([ True, False,  True], dtype=bool)
  

 

 The problem with  np.equal.reduce  can be seen by micro-analyzing what happens when it is applied to  [1, 0, 0, 1] : 

  In [49]: np.equal.reduce([1, 0, 0, 1])
Out[50]: True
  

 The first two items,  1  and  0  are tested for equality and the result is  False : 

  In [51]: np.equal.reduce([False, 0, 1])
Out[51]: True
  

 Now  False  and  0  are tested for equality and the result is  True : 

  In [52]: np.equal.reduce([True, 1])
Out[52]: True
  

 But  True  and 1 are equal, so the total result is  True , which is not the desired outcome. 

 The problem is that  reduce  tries to accumulate the result "locally", while we want a "global" test like  np.all . 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/41209456)
 Here's a vectorized approach using https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html - 

  # Get mask of matching elements against the iterators
m,n = I.shape
Imask = I == np.arange(m)[:,None,None,None]
Jmask = J == np.arange(n)[:,None,None]

# Get the mask of intersecting ones
mask = Imask & Jmask

# Get D intersection masked array
Dvals = np.where(mask,D,np.inf)

# Get argmin along merged last two axes. Index into flattened V for final o/p
out = V.ravel()[Dvals.reshape(m,n,-1).argmin(-1)]
  

 Sample input, output - 

  In [136]: I = np.array([[0,1,2],[1,1,0],[0,0,2]])
     ...: J = np.array([[1,1,1],[1,2,1],[0,1,0]])
     ...: D = np.array([[1.2, 3.4, 2.2],[2.2, 4.3, 2.3],[7.1, 6.1, 2.7]])
     ...: V = np.array([[1.1 , 8.1, 9.1],[3.1, 7.1, 2.1],[0.1, 5.1, 3.1]])
     ...: 

In [144]: out
Out[144]: 
array([[ 0.1,  1.1,  1.1], # To verify : v[0,1] = 1.1
       [ 1.1,  3.1,  7.1],
       [ 3.1,  9.1,  1.1]])
  


-------------------------------------next answer-------------------------------------

Rank:6 (https://stackoverflow.com/questions/19161693)
 One solution is to index the rows/columns by slicing/striding. Here's an example where you are extracting every third column/row from the first to last columns (i.e. the first and fourth columns) 

  In [1]: import numpy as np
In [2]: Y = np.arange(16).reshape(4, 4)
In [3]: Y[0:4:3, 0:4:3]
Out[1]: array([[ 0,  3],
               [12, 15]])
  

 This gives you the output you were looking for. 

 For more info, check out http://docs.scipy.org/doc/numpy/user/basics.indexing.html.  


-------------------------------------next answer-------------------------------------

Rank:7 (https://stackoverflow.com/questions/37332477)
 You could simply loop through the columns of  df  and build a Series out of the correlations: 

  result = (pd.Series([df['m'].corr(df[col]) for col in df], index=df.columns)
          .abs().sort_values())
  

 If you plan to do this for many columns, it may be quicker to calculate the entire correlation matrix and use  .loc  to select the rows of interest:  so.loc['m'] . 

 

 For example,  

  import numpy as np
import pandas as pd
np.random.seed(2016)

df = pd.DataFrame(np.random.random((4,4)), columns=list('klmn'))
result = (pd.Series([df['m'].corr(df[col]) for col in df], index=df.columns)
          .abs().sort_values())
print(result)
# l    0.041438
# n    0.086255
# k    0.393375
# m    1.000000
# dtype: float64

c = df.corr().abs()
s = c.unstack()
so = s.sort_values()  # s.order is deprecated. use s.sort_values
print(so.loc['m'])
# l    0.041438
# n    0.086255
# k    0.393375
# m    1.000000
# dtype: float64
  

 Note  result  and  so.loc['m']  are the same. 


-------------------------------------next answer-------------------------------------

Rank:8 (https://stackoverflow.com/questions/50775598)
 With  masking . We first generate a matrix with the same dimensions, of random numbers, and check if these are larger than  0.8 : 

  mask = np.random.random(a.shape) > 0.8
  

 Now we can assign the values of  b  where  mask  is  True  to the corresponding indices of  a : 

  a[mask] = b[mask]
  

 For example: 

  >>> a
array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]])
>>> b
array([[2, 3, 4, 5],
       [6, 7, 8, 9],
       [0, 2, 3, 4]])
>>> mask = np.random.random(a.shape) > 0.8
>>> mask
array([[ True, False, False, False],
       [ True, False, False, False],
       [False, False, False, False]])
>>> a[mask] = b[mask]
>>> a
array([[2., 1., 1., 1.],
       [6., 1., 1., 1.],
       [1., 1., 1., 1.]])
  

 So here where the  mask  is  True  (since  0.8  is rather high, we expect on average 2.4 such values), we assign the corresponding value of  b . 


-------------------------------------next answer-------------------------------------

Rank:9 (https://stackoverflow.com/questions/5371300)
 Check out sympy -- it does quite a good job at polymorphism
in its matrices and you you have operations on sympy.matrices.Matrix
objects like col_swap, col_insert, col_del, etc... 

 
In [2]: import sympy as s 
In [6]: import numpy as np

In [11]: npM = np.array([[1,2,3.0], [4,4,"abc"]], dtype=object)
In [12]: npM
Out[12]: 
 [[1 2 3.0]
 [4 4 abc]]

In [14]: type( npM[0][0] )
Out[14]: 
In [15]: type( npM[0][2] )
Out[15]: 
In [16]: type( npM[1][2] )
Out[16]: 


In [17]: M = s.matrices.Matrix(npM)
In [18]: M
Out[18]: 
⎡1  2  3.0⎤
⎢         ⎥
⎣4  4  abc⎦


In [27]: type( M[0,2] )
Out[27]: 
In [28]: type( M[1,2] )
Out[28]: 

In [29]: sym= M[1,2] 
In [32]: print sym.name
abc

In [34]: sym.n
Out[34]: 
In [40]: sym.n(subs={'abc':45} )
Out[40]: 45.0000000000000

 


-------------------------------------next answer-------------------------------------

Rank:10 (https://stackoverflow.com/questions/40859284)
 consider the dataframe  df  

  np.random.seed([3,1415])
df = pd.DataFrame(np.random.rand(3, 3), list('abc'), list('xyz'))
df
  

 https://i.stack.imgur.com/5bYWg.png 

 calculate the inverse (with numpy, let's not be crazy) 

  df_inv = pd.DataFrame(np.linalg.pinv(df.values), df.columns, df.index)
df_inv
  

 <a href="https://i.stack.imgur.com/jcs9v.png"  

 notice I use  pinv  for the pseudo inverse 

 then check 

  df_inv.dot(df)
  

 <a href="https://i.stack.imgur.com/mY4H3.png"  



